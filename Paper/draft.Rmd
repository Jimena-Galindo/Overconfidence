---
title: "Learning from Data Through Models"  # nolint: Grammarly.
date: '`r format(Sys.time(), "%B %d, %Y")`'
author: "Jimena Galindo"
abstract: "TBW"
#classoption: pagebackref # passes pagebackref=true to hyperref before it is loaded, allowing backreferencing
output: 
    pdf_document:
        citation_package: natbib
        fig_width: 7
        fig_height: 6
        fig_caption: true
        number_sections: true
        
        template: NULL
        keep_tex: true
        extra_dependencies:
            footmisc: ["bottom"] # footnote management 
            setspace: ["doublespacing"] # spacing of the paper 
            caption: ["normal"] 
            dsfont: null # indicator function 1
            booktabs: null 
            makecell: null 
            hyperref: null 
        includes:
          in_header: extra_header.tex

bibliography: references.bib
fontsize: 12pt 
geometry: margin=2.5cm
---

```{r dependencies, include=FALSE}
source("../Paper/parameters_and_packages.R")
```

# Introduction


# Framework

REWRITE IN GENERAL TERMS WITH THE PROBABILITY BEING GIVEN BY A FUNCTION THAT SATIEFIES ALL THE PROPERTIES IN BA SO THAT I CAN 
REFER TO HER RESULTS.
THEN EXPAND ON THE EXAMPLE FOR THE LAB AND SAY WHY IT CAPTURES ALL THE FORCES WE WANT.

An agent is of type $\theta \in \{\theta_L, \theta_M, \theta_H\}$  with $\theta_H > \theta_M > \theta_L$ and they face an unknown 
exogenous state
$\omega \in \{\omega_L, \omega_M, \omega_H\}$ with $\omega_H>\omega_M>\omega_L$. 
Each of the values of $\omega$ is realized with equal probability. The agent knows the distribution of $\omega$ but not its value. 
They also hold some prior belief about $\theta$ which is potentially misspecified.\footnote{The particular types of misspecification
that are allowed are discussed below.}
The agent chooses a binary gamble $e in \{e_L, e_M, e_H\}$ and observes whether the gamble is a success or a failure. '
If it is a success,
the agent gets a payoff of $1$ and if it is a failure, the agent gets a payoff of $0$. The probability of success is increasing in
both $\theta$ and $\omega$ and is fully described in the following table:

\begin{tabular}{ c|c|c|c|}
  
  \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\omega_H$} & \multicolumn{1}{c}{$\omega_M$} & \multicolumn{1}{c}{$\omega_L$}\\
  \cline{2-4}
  $e_H$ & 50 & 20 & 2 \\
  \cline{2-4}
  $e_M$ & 45 & 30 & 7 \\
  \cline{2-4}
  $e_L$ & 40 & 25 & 20 \\
  \cline{2-4}
  \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\theta_L$} & \multicolumn{1}{c}{}\\
\end{tabular}
\hspace{.3cm} 
\begin{tabular}{ c|c|c|c|}
  
  \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\omega_H$} & \multicolumn{1}{c}{$\omega_M$} & \multicolumn{1}{c}{$\omega_L$}\\
  \cline{2-4}
  $e_H$ & 80 & 50 & 5 \\
  \cline{2-4}
  $e_M$ & 69 & 65 & 30 \\
  \cline{2-4}
  $e_L$ & 65 & 45 & 40 \\
  \cline{2-4}
  \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\theta_M$} & \multicolumn{1}{c}{}\\
\end{tabular}
\hspace{.3cm} 
\begin{tabular}{ c|c|c|c|}
  
  \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\omega_H$} & \multicolumn{1}{c}{$\omega_M$} & \multicolumn{1}{c}{$\omega_L$}\\
  \cline{2-4}
  $e_H$ & 98 & 65 & 25 \\
  \cline{2-4}
  $e_M$ & 80 & 69 & 35 \\
  \cline{2-4}
  $e_L$ & 75 & 55 & 45 \\
  \cline{2-4}
  \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\theta_H$} & \multicolumn{1}{c}{}\\
\end{tabular}

Conditional on a type, the agent's flow payoff is maximized by choosing the gamble that 
matches the state. For example, if the value of $\omega$ is $\omega_H$, the agent's flow payoff is maximized by 
choosing $e_H$ and if the state is $\omega_L$ the flow payoff is maximized by choosing gamble $e_L$. The agent myopically chooses
gambles every period to maximize the flow payoff for $T<\infty$ periods, and the agent's payoff is the sum of the payoffs from 
each period. 

After observing the outcome of each gamble, the agent updates their beliefs of both theta and omega in one of 4 different ways: 
As a \emph{dogmatic} as in @Heidhues2018 ; as a \emph{switcher} as in @Ba2023 ; as a \emph{self-attribution} updater as in @Coutts2020; 
or as a fully Bayesian agent. Each of these updating rules is described in detail below.

Notice that both $\theta$ and $\omega$ can be identified from the outcomes if enough variation in the effort choices exists.
This can be seen by confirming that no two columns are the same. Therefore, the agent can learn both their type and the 
state by observing enough outcomes of the gambles for different effort choices.

## The Bayesian Benchmark

A Bayesian agent simultaneously updates their beliefs about $\theta$ and $\omega$ by using Bayes' rule. 
The posterior odds at period t about $\theta$ after observing an outcome are given by:
\begin{equation}
\frac{p_{t}[\theta_H|\text{outcome}]}{p_{t}[\theta_M|\text{outcome}]} = 
      \frac{p[\text{outcome}|\theta_H]p_{t-1}[\theta_H]}{p[\text{outcome}|\theta_M]p_{t-1}[\theta_M]}
\end{equation}
and
\begin{equation}
\frac{p_{t}[\theta_M|\text{outcome}]}{p_{t}[\theta_L|\text{outcome}]} = 
      \frac{p[\text{outcome}|\theta_M]p_{t-1}[\theta_M]}{p[\text{outcome}|\theta_L]p_{t-1}[\theta_L]}
\end{equation}

Where $p_{t-1}$ is the prior at period $t$ and 
$p[\text{outcome}|\theta] = \sum_{\omega} p[\text{outcome}|\theta, \omega, e]p_{t-1}(\omega)$
is the probability of observing the outcome given the agent's type and the effort chosen. The update is symmetric for $\omega$.

Bayesian agents always choose the effort level that maximizes their flow payoff by taking expectations over
their prior beliefs about $\theta$ and $\omega$. Since agents are myopic, even though all the parameters could be identified with enough variation in choices, 
a fully Bayesian agent might not learn their true type. This happens because they do not internalize the tradeoff between flow payoff
and learning and thus might not experiment enough to learn their type. An alternative to this approach is given 
by @Hestermann2021 and is discussed with the results.

## A Biased Agent
An agent that updates their beliefs with self-attribution bias will update their beliefs about the state $\omega$ and their type $\theta$
by over-attributing successes to a high value of $\theta$ and under-estimating the role of higher $\omega$. Similarly,
they will attribute failure to a low state more than an unbiased agent would. 
To model the self-serving attribution bias, I take the approach of @benjamin2019, where a generalization of the Bayes rule above gives the update.

\begin{equation}
\frac{p_{t}[\theta_H|\text{outcome}]}{p_{t}[\theta_M|\text{outcome}]} = 
      \left(\frac{p[\text{outcome}|\theta_H]}{p[\text{outcome}|\theta_M]}\right)^{c_s^{\theta}\mathbb{I}\{\text{success}\}+c_f^{\theta}\mathbb{I}\{\text{failure}\}}\frac{p_{t-1}[\theta_H]}{p_{t-1}[\theta_M]}
\end{equation}
and
\begin{equation}
\frac{p_{t}[\theta_M|\text{outcome}]}{p_{t}[\theta_L|\text{outcome}]} = 
      \left(\frac{p[\text{outcome}|\theta_M]}{p[\text{outcome}|\theta_L]}\right)^{c_s^{\theta}\mathbb{I}\{\text{success}\}+c_f^{\theta}\mathbb{I}\{\text{failure}\}}\frac{p_{t-1}[\theta_M]}{p_{t-1}[\theta_L]}
\end{equation}

$c_s^{\theta}$ and $c_f^{\theta}$ are the self-serving attribution bias parameters for the agent's type $\theta$. 
If $c_s^{\theta} = c_f^{\theta} = 1$, the agent is unbiased and the update is the same as the Bayesian update. On the other hand, 
if $c_s^{\theta} > c_f{\theta}$ the agent over-attributes success to their type and under-attributes failure to their type\footnote{
  notice that the values of $c_s^{\theta}$ and $c_f^{\theta}$ are not restricted to be greater than 1. If they are both equal to 
  each other but less (more) than one, then the bias is simply underinference (overinference).}.

The update for $\omega$ is analogous but with $c_f^{\omega}$ and $c_s^{\omega}$ instead of $c_f^{\theta}$ and $c_s^{\theta}$ and
 the bias is present whenever $c_f^{\omega} > c_s^{\omega}$. That is, the agent over-attributes failure to a low state relative the 
higher states and under-attributes success to a low state relative to the higher states.

## The Dogmatic Agent

A dogmatic agent does not update their beliefs about $\theta$; instead,
they hold a degenerate belief that places probability one on being a particular type, $\hat{\theta}$. For instance, the dogmatic agent 
might be of type $\hat{\theta} = \theta_M$ but holds a belief that places probability one on being of type $\theta_H$. In this case, no matter
how much information he gathers against being of type $\theta_H$, he will not update his beliefs. Any discrepancies between the 
observed outcomes are incorporated using the Bayes rule to update their beliefs about $\omega$. This results in the following
posterior odds:

\begin{equation}
\frac{p_{t}[\omega_H|\text{outcome}]}{p_{t}[\omega_M|\text{outcome}]} = 
      \frac{p[\text{outcome}|\omega_H, \hat{\theta}, e]p_{t-1}[\omega_H]}{p[\text{outcome}|\omega_M, \hat{\theta}, e]p_{t-1}[\omega_M]}
\end{equation}
and
\begin{equation}
\frac{p_{t}[\omega_M|\text{outcome}]}{p_{t}[\omega_L|\text{outcome}]} = 
      \frac{p[\text{outcome}|\omega_M, \hat{\theta}, e]p_{t-1}[\omega_M]}{p[\text{outcome}|\omega_L, \hat{\theta}, e]p_{t-1}[\omega_L]}
\end{equation}

A crucial difference between the dogmatic agent and the Bayesian agent is that the dogmatic agent does not aggregate across 
types when updating their beliefs about $\omega$. This means the dogmatic agent will never learn their true type if they are 
misspecified.

@Heidhues2018 show that in a setting such as ours, a dogmatic modeler will inevitably fall into a self-confirming equilibrium where
the outcomes they observe reinforce their belief on $\omega$ in such a way that as $t\to\infty$ the agent will be certain that
the state is some $\omega^*$ consistent with their believed type and the observed data.

## The Switcher 

An agent is a \emph{switcher} if they behave as a dogmatic but is willing to entertain the possibility that they are of a different type.
In particular, when they start off as a misspecified dogmatic, they are willing to switch to a different dogmatic belief if the data is
convincing enough. 

In order to abandon their initial dogmatic belief, the agent needs to observe a sequence of outcomes that are sufficiently unlikely
to have happened if they were of the type they initially believed. They do so by keeping track of the likelihood that each of the
possible types generated the data. If the likelihood ratio is sufficiently large, the agent will switch to the alternative
and behave as if they are dogmatic about the new type.

In particular, for an agent that starts off with a dogmatic belief that they are of type $\hat{\theta}$ but is willing to consider
the alternative explanation that they are of type $\tilde{\theta}$, the agent will switch to the alternative if:

$$\frac{p[h^t|\tilde{\theta}]}{p[h^t|\hat{\theta}]} > \alpha$$

Where $h^t$ is the history of outcomes up to time $t$ and $\alpha \geq 1$ is a threshold that determines how convincing the data needs to be
for the agent to change their belief. Notice that if $\alpha \to \infty$, we get the dogmatic agent. In this sense, the 
switcher is a generalization of the dogmatic type, just as the self-attribution agent is a generalization of the Bayesian agent.

By allowing the agent to keep track of the likelihoods and switching to an alternative type, the switcher can avoid the 
self-confirming equilibrium the dogmatic agent falls into. However, if the prior belief on $theta$ is sufficiently tight around
a self-confirming equilibrium, the switcher might look identical to the dogmatic even in a case where $\alpha$ is not too large.

# Experimental Design




# Analysis




# Conclusion