---
title: "Learning with misspecified models: Overconfidence and Stereotypes"  # nolint: Grammarly.
date: '`r format(Sys.time(), "%B %d, %Y")`'
author: "Jimena Galindo"
abstract: "TBW"
#classoption: pagebackref # passes pagebackref=true to hyperref before it is loaded, allowing backreferencing
output: 
    pdf_document:
        citation_package: natbib
        fig_width: 7  
        fig_height: 6
        fig_caption: true
        number_sections: true
        
        template: NULL
        keep_tex: true
        extra_dependencies:
            footmisc: ["bottom"] # footnote management 
            setspace: ["doublespacing"] # spacing of the paper 
            caption: ["normal"] 
            dsfont: null # indicator function 1
            booktabs: null 
            makecell: null 
            hyperref: null 
        includes:
          in_header: extra_header.tex

bibliography: references.bib
fontsize: 12pt 
geometry: margin=2.5cm
---

```{r dependencies, include=FALSE}
source("../Paper/parameters_and_packages.R")
```

# Introduction


## Related literature


# Theoretical Framework

I consider multiple theories of belief updating that have been proposed in the literature and that are able to rationalize
the prevalence of misspecified beliefs. I focus on two different frameworks and develop a unifying example that allows me to
compare the predictions of all the theories. In this section, I outline the theories within each of the frameworks.

## Framework 1

An agent is of type $\theta \in \Theta$ and faces an unknown exogenous state $\omega$ drawn from some 
density $f$ over $\Omega$. The agent knows the 
distribution of $\omega$ but not its realized value. His prior belief about his type is $p_0(\theta)$ and his belief about the state, 
$p_0(\omega)$, coincides with the true distribution. Let the agent's true type be $\theta^{*}$,  and the realized state be $\omega^{*}$.

An agent has a \emph{misspecified} belief if the prior assigns probability zero to their true type. Furthermore, the agent is 
\emph{dogmatic} if he holds a degenerate belief that places probability one on being a particular type, $\hat{\theta}$. An 
agent can be dogmatic and misspecified. If the agent is both, $\hat{\theta} \neq \theta^*$ and $p_0(\hat{\theta}) = 1$.

The agent chooses an action $a\in A$ and observes a noisy outcome $h\in H$. 
The outcome is a function of the agent's type, the state, and
the action. In particular $h = h(\theta^*, \omega^*, a) + \varepsilon$ with $h(\cdot)$ increasing in both $\theta^*$ and $\omega^*$, 
and such that conditional on a pair of parameters $(\theta, \omega)$, there is a unique optimal action. 
$\varepsilon\sim N(0, \sigma)$ is noise in the output.  
 
After observing the outcome, the agent updates his beliefs about $\theta$ and $\omega$ using some algorithm and moves on to the next period.
He repeats this process infinitely many times. I make the simplifying assumption that the agent is myopic and chooses the action 
that maximizes the payoff in each period. This assumption simplifies the analysis and plays a role in whether an agent who updates
 their beliefs using Bayes rule would learn the truth or not. The behavior of a forward-looking agent is discussed in the conclusion.


A key notion in this setting is that of a self-defeating equilibrium\footnote{This notion is an adaptation of the Berk-Nash equilibrium in 
@Esponda2016 to this setting with only one agent}. 
A \emph{self-defeating equilibrium} is a belief and action pair such that the agent's belief about their type is misspecified and the 
outcome generated by the action is consistent with the misspecified belief. This means that the average outcome under the true type and the true 
state equals the average output the agent expects under the misspecified belief. The agent's belief is said to be 
\emph{stable} when this happens.

Within this framework, I consider two nested theories of belief updating: the first one is a dogmatic modeler from @Heidhues2018,
The second one is a switcher, as in @Ba2023. The dogmatic modeler can be seen as a switcher with an infinitely sticky initial belief
and this is the sense in which the two are nested. Both theories produce different predictions about the agent's behavior. I discuss both 
in what follows.

### The Dogmatic Modeler

A dogmatic agent does not update their beliefs about $\theta$, instead, he holds a degenerate belief that places probability on 
$\hat{\theta}$, which is potentially misspecified. 
In this case, no matter
how much information he gathers against being of type $\hat{\theta}$, he will not update his beliefs about $\theta$. 
Any discrepancies between the 
observed outcomes and his believed type are incorporated using the Bayes rule to update their beliefs about $\omega$.
@Heidhues2018 show that, under certain assumptions on the per-period utility,\footnote{The assumptions are that 
$u$ is twice continuously differentiable with: (i)$u_{ee}<0$ and $u_e(\underline{e} \theta, \omega)>0>u_e(\Bar{e}, \theta, \omega)$, 
(ii) $u_{\theta}, u_{\omega}>0$ and (iii) $u_{e\theta}<0$ and $u_{e\omega}>0$. The direction of the derivatives is a normalization
and the results would hold even when the signs are reversed.} 
a dogmatic modeler will inevitably fall into a self-defeating equilibrium. The equilibrium will be such that
the outcomes they observe reinforce their belief on $\omega$ in such a way that as $t\to\infty$ the agent will 
be sure that
the state is some $\omega^{'}$ consistent with their believed type and the observed data. In other words, 
they will be in a self-defeating equilibrium with a stable belief that places probability one on the incorrect
parameters $(\hat{\theta}, \omega_{\infty})$

The mechanism by which the dogmatic agent falls into the self-defeating equilibrium is the following: Suppose the agent holds the 
misspecified belief that they are type $\hat{\theta}>\theta^*$. For any prior over $\omega$, the agent will be disappointed by the outcome.
He expected a gain of $h(\hat{\theta}, \mathbb{E}(\omega), a)$ but instead observes  $h(\hat{\theta}, \omega^*, a)$. 
There are two 
possible sources for the disappointment, the first is that the realized state is lower than the expected state. The second source is that the
agent is of type $\theta^*$ and therefore, for all possible states, his gain will be lower than what he expected. Because the agent is dogmatic,
he will not update his beliefs about $\theta$ and as a consequence will attribute the disappointment to the state being lower than expected.
He will continue to update in this way until he converges to a belief about $\omega$ that is stable. Such a belief will 
explain the observed utility perfectly and allow the agent to rationalize his dogmatic belief about $\theta$. 
Under the assumptions of @Heidhues2018, there is a unique value of $\omega$ at which the belief is stable, I will refer to such value
as $\omega_\infty$.
This mechanism is further illustrated in Example 1.

**Example 1: ** 
_Set $A = \Omega$ and $H = [0, \infty)$ and consider a student with intrinsic ability $\theta^*\geq 0$ who faces a grading procedure 
$\omega^*$ that is unknown to them. However, 
they know that a higher $\omega^*$ is more likely to yield a higher grade. In particular, assume the grade is given by 
$(\theta^*+a)\omega^*$._

_The student must choose an effort level $a$, which determines their grade. For whatever the chosen effort level, 
the agent must pay a cost $c(a) = \frac{1}{2}a^2$. And he repeats this process for infinitely many periods. 
Assume also that the student's prior is such that $mathbb{E}[\omega]= \omega^*$ and he is dogmatic about being of 
type $\hat{\theta}>\theta^*$. \footnote{The example is illustrated for an overconfident agent but the results are symmetric for a digmatic agent who initially 
places probability one on some $\tilde{\theta}<\theta^*$.}
Therefore, the student's payoff in period $t$ is given by _

\begin{equation}
u_t(a_t; \theta^*, \omega^*) = (\theta^*+a_t)\omega^* - \frac{1}{2}a^2 + \varepsilon_t
\end{equation}

_Under this specification, the myopic optimal effort level is $a_t^* = \omega^*$. Nonetheless, because the agent does not know $\omega^*$,
he will choose $a_t = \mathbb{E}_t(\omega)$ where the expectation is taken with respect to the agent's belief at the beginning of
period $t$. If he does not revise his effort choice for $k$ periods, he will receive 
an average utility of 
$(\theta^*+a_t^*)\omega^* - \frac{1}{2}a_t^{*2}$ but he was expecting an average utility of 
$(\hat{\theta}\theta+a_t^*)\omega^* - \frac{1}{2}a_t^{*2}$. In response, he will apply Bayes rule to update his beliefs about
$\omega$ to get the posterior belief with $\mathbb{E}_{t+k}[\omega] = \frac{(\theta^{*} + \omega^{*})\omega^{*}}{\hat{\theta} + \omega^{*}}$
which is lower than the initial belief. This will cause the agent to choose a lower effort at $t+k$. As a result, he will again
receive an average utility that is lower than what he expected which will cause his belief to drift further down. This process will
continue until the average utility equals his expected utility under the dogmatic belief that assigns probability 1 to $\hat{\theta}$. 
At that point, the student will have reached a self-defeating equilibrium and he will continue to choose sub-optimal effort forever. _

Although the model of a dogmatic modeler rationalizes the prevalence of overconfident (underconfident) beliefs, the
assumption that the agent has a degenerate belief and no mechanism through which he can update such belief is very restrictive. An
alternative approach is proposed by @Ba2023. She proposes an extension of the dogmatic agent who is able to jump from one dogmatic 
belief to another. By doing so, the agent might end up being dogmatic and correctly specified.

### The Switcher 

An agent is a \emph{switcher} if they behave as a dogmatic, but is willing to entertain the possibility that they are of a different type.
In particular, when they start off as a misspecified dogmatic, they are willing to switch to a different dogmatic belief if the data is
convincing enough. Their prior is still degenerate and assigns probability one to a particular type, and zero to 
all other types. This means that a Bayesian update on $\theta$ does not change their beliefs about the type.
However, they are willing to entertain two such beliefs and have a mechanism by which they decide which belief to adopt at any period $t$.

In order to abandon their initial dogmatic belief, the agent needs to observe a sequence of outcomes 
that are sufficiently unlikely
to have happened if they were of the type they initially believed. In order to evaluate if the evidence is convincing enough, 
they keep track of the likelihood that each of the
possible types generated the data. If the likelihood ratio is sufficiently large, the agent will switch to the alternative
and behave as if they are dogmatic about the new type.

In particular, for an agent that starts with a dogmatic belief that they are of type $\hat{\theta}$ but is willing to consider
the alternative explanation that they are of type $\tilde{\theta}$, the agent will switch to the alternative if:

$$\frac{p[h^t|\tilde{\theta}]}{p[h^t|\hat{\theta}]} > \alpha\geq 1$$

Where $h^t$ is the history of outcomes up to time $t$ and $\alpha$ is the switching threshold. By keeping track of the 
likelihood ratio, the agent can perform a \emph{Bayesian hypothesis test} and adopt the Dogmatic belief that best fits 
the data.\footnote{In a related problem @Schwarstein2021 proposes a similar updating procedure which relies on the Bayesian 
hypothesis test. However, in their model there is a sender who optimally chooses to propose a model that fits the data}
Notice that if $\alpha \to \infty$, the behavior of the switcher will be indistinguishable from that of the Dogmatic modeler. 
In this sense, the switcher is a generalization of the dogmatic type.

By allowing the agent to keep track of the likelihoods and switching to an alternative type, the switcher can avoid the 
self-confirming equilibrium. However, if the prior belief on $omega$ is sufficiently tight around
a self-defeating equilibrium, the switcher might look identical to the dogmatic even in a case where $\alpha$ is not too large.
This happens because under the agent's prior, the likelihood ratio is unlikely to grow as fast as it is needed to escape 
the self-defeating equilibrium. In such situations, we say that the misspecified belief is persistent. 

## Framework 2

As in framework 1, the agent is of some type $\theta^* \in \Theta$ and the state is $\omega \sim F$. 
In this case, the agent chooses an action 
$a\in A$
and observes a binary outcome that is either a success or a failure. Denote the outcome by $o \in {s,f}$. The probability of 
observing a success is increasing in $\theta^*$ and in $\omega$. Whenever the agent observes a success, he gets a payoff $v>0$ and
whenever the outcome is a failure, the payoff is 0.  In addition, the probability of success is such that for each state, 
there is a unique optimal action that maximizes the agent's expected payoff.

I focus on two nested theories that have been widely studied within this framework: Full Bayesian updating and self-serving attribution 
bias. I explain each of these classical models of belief updating in what follows


### The Bayesian

A Bayesian agent simultaneously updates their beliefs about $\theta$ and $\omega$ by using Bayes' rule. 
The posterior odds at period t about $\theta$ after observing an outcome are given by:
$$
\frac{p_{t}[\theta_H|\text{outcome}]}{p_{t}[\theta_M|\text{outcome}]} = 
      \frac{p[\text{outcome}|\theta_H]p_{t-1}[\theta_H]}{p[\text{outcome}|\theta_M]p_{t-1}[\theta_M]}
$$
and
$$
\frac{p_{t}[\theta_M|\text{outcome}]}{p_{t}[\theta_L|\text{outcome}]} = 
      \frac{p[\text{outcome}|\theta_M]p_{t-1}[\theta_M]}{p[\text{outcome}|\theta_L]p_{t-1}[\theta_L]}
$$

Where $p_{t-1}$ is the prior at period $t$ and 
$p[\text{outcome}|\theta] = \sum_{\omega} p[\text{outcome}|\theta, \omega, e]p_{t-1}(\omega)$
is the probability of observing the outcome given the agent's type and the effort chosen. The update is symmetric for $\omega$.

Bayesian agents always choose the effort level that maximizes their flow payoff by taking expectations over
their prior beliefs about $\theta$ and $\omega$. Since agents are myopic, even though all the parameters 
could be identified with enough variation in choices, although the Bayesian agent is the closest to a fully rational agent discussed here, 
they might not learn their true type. This happens because, by being myopic, they do not internalize the tradeoff between flow payoff
and learning. This can result in too little experimentation to learn their true type. An alternative to this approach is given 
by @Hestermann2021 and is discussed with the results.

### The Self-Serving Updater

A self-serving bayesian is an agent who uses a biased version of Bayes rule to update their beliefs. 
They will update their beliefs 
about the state $\omega$ and his type $\theta$ simultaneously
by over-attributing successes to a high value of $\theta$ and under-estimating the role of higher $\omega$. Similarly,
he will attribute failure to a low state to a greter degree than an unbiased agent would. 
To model the self-serving attribution bias, I take the approach of @benjamin2019, where the posterior odds are given by:

$$
p_{t+1}[\theta, \omega|s_t] =\left(
\frac{p_t[s_t|\theta, \omega]}
{p_t[s_t|\theta', \omega']}\right)^{c(\theta, \omega, s_t)}
$$

Whith $c(\theta, \omega, s_t)$ such that   
c(\theta, \omega, \s_t) > c(\theta', \omega').


# A Unifying Example
The agent can be of one of 3 types: $\theta \in \{\theta_L, \theta_M, \theta_H\}$  with $\theta_H > \theta_M > \theta_L$.
They face an unknown exogenous success rate
$\omega \in \{\omega_L, \omega_M, \omega_H\}$ with $\omega_H>\omega_M>\omega_L$. Each of the values of $\omega$ is realized with 
equal probability. 
The agent knows the distribution of $\omega$ but not its realized value.

Denote the true type by $\theta^*$ and the true state by $\omega^*$. The agent holds some prior belief about $\theta$ 
\footnote{which is potentially misspecified as in the dogmatic and switcher cases discussed above} 
and chooses a binary gamble $e in \{e_L, e_M, e_H\}$. The agent observes whether the gamble is a success or a failure and 
gets a payoff of $1$ and if it is a success; they get $0$ otherwise. 

The probability of success is increasing in both $\theta$ and $\omega$ and is fully described by the following table:

\begin{tabular}{ c|c|c|c|}
  
  \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\omega_H$} & \multicolumn{1}{c}{$\omega_M$} & \multicolumn{1}{c}{$\omega_L$}\\
  \cline{2-4}
  $e_H$ & 50 & 20 & 2 \\
  \cline{2-4}
  $e_M$ & 45 & 30 & 7 \\
  \cline{2-4}
  $e_L$ & 40 & 25 & 20 \\

  \cline{2-4}
  \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\theta_L$} & \multicolumn{1}{c}{}\\
\end{tabular}
\hspace{.3cm} 
\begin{tabular}{ c|c|c|c|}
  
  \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\omega_H$} & \multicolumn{1}{c}{$\omega_M$} & \multicolumn{1}{c}{$\omega_L$}\\
  \cline{2-4}
  $e_H$ & 80 & 50 & 5 \\
  \cline{2-4}
  $e_M$ & 69 & 65 & 30 \\
  \cline{2-4}
  $e_L$ & 65 & 45 & 40 \\
  \cline{2-4}
  \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\theta_M$} & \multicolumn{1}{c}{}\\
\end{tabular}
\hspace{.3cm} 
\begin{tabular}{ c|c|c|c|}
  
  \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\omega_H$} & \multicolumn{1}{c}{$\omega_M$} & \multicolumn{1}{c}{$\omega_L$}\\
  \cline{2-4}
  $e_H$ & 98 & 65 & 25 \\
  \cline{2-4}
  $e_M$ & 80 & 69 & 35 \\
  \cline{2-4}
  $e_L$ & 75 & 55 & 45 \\
  \cline{2-4}
  \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\theta_H$} & \multicolumn{1}{c}{}\\
\end{tabular}

Conditional on a type, the agent's flow payoff is maximized by choosing the gamble that 
matches the state. For example, if the value of $\omega$ is $\omega_H$, the agent's flow payoff is maximized by 
choosing $e_H$ and if the state is $\omega_L$ the flow payoff is maximized by choosing gamble $e_L$, regardless of the value of $\theta$. 
The agent myopically chooses gambles every period to maximize the flow payoff for $T<\infty$ periods. 

After observing the outcome of each gamble, the agent updates their beliefs using some procedure and moves on to the next period.

Notice that both $\theta$ and $\omega$ can be identified from the outcomes if enough variation in the effort choices exists.
This can be seen by confirming that there is no pair of $\theta$ and $\omega$ such that the probability of success is the same for all effort 
choices. Thus, by changing the effort choice, the agent can learn both their type and the state if they observe enough outcomes.

In this example, for an agent with a dogmatic belief about their type, a self-defeating equilibrium is one in which the agent 
chooses an effort level that, under the true $\theta$, yields a frequency of success that is consistent with the agent's 
misspecified belief. That is $P[\text{sucess}|\theta^*, e^*] = P[\text{sucess}|\hat{\theta}, e^*]$ where $e^*$ is the agent's 
myopic optimal choice.

In the data-generating process described above, there are five such equilibria. For example, if the agent is of type $\theta_M$  but
mistakenly believes that he is of type $\hat{\theta}=\theta_H$ and the 
and $\omega^* = \omega_M$, when the effort chosen is $e_L$, the agent will observe a success with 45% chance. Because the agent 
dogmatically believes that their type is high, they will erroneously conclude that the rate is $\omega_L$. Under this belief,
the optimal action is $e_L$ which will continue to generate successes with $45%$ probability, further reinforcing
the incorrect belief. By doing so, the agent forgoes the payoff from gamble $e_M$ which would yield a success with $65\%$ chance.

By including self-confirming equilibria, the example captures the forces from each of the updating mechanisms discussed in the 
previous section and allows for direct comparison of all the theories. For realizations of $(\theta, \omega)$ for which there are 
self-confirming equilibria, the dogmatic agent will fall into the 
trap whereas the switcher will be able to escape it. Similarly, an agent with self-attribution bias will update their beliefs differently from an unbiased Bayesian, leading
them to choose different gambles. I exploit such cases in order to test which model is a better fit for how subjects behave in a 
laboratory experiment. \footnote{because the setting does not match that of Heidhues et al. [2018], there will be situationf for which the theory 
does not provie a prediction. If such cases arise in the lab, they will not be used for the analysis. However, whether a misspecified 
belief persists or not for the switcher, depends highly on the realized history of signals that he gets.} In what follows I explain 
the details of how this example was implemented in the lab.


# Experimental Design

I recruited XXX undergraduate subjects from the CESS lab at NYU who participated in an in-person experiment. Sessions lasted approximately 
XXX hours and subjects earned an average payment of XXX. The experiment was programmed using oTree [@chen2016otree].

The experiment consisted of 2 treatments: the \emph{ego-relevant} condition and the \emph{stereotype} condition. Subjects participated 
in only one of the treatments. Treatments were randomly assigned at the session level. The tasks were identical across treatments, 
except for parameter $\theta$. In the ego-relevant condition $\theta$ is the subject's own performance in a quiz, while in the 
stereotype condition, it is the performance of a randomly selected subject from another session.

The experiment had 3 parts. In Part 1 subjects had 2 minutes to answer as many multiple-choice questions as they could from a 
20-question quiz. They did this for quizzes on 6 different topics. The topics were: Math, Verbal 
Reasoning, Pop-culture and Art, Science and Technology, US Geography, and Sports and Video Games. 
In this part, they did not know how many questions were available and they were given no feedback.

After taking all 6 quizzes, they proceeded to part 2 where they were asked to guess their score on each of them. In the stereotype 
treatment they were additionally asked to guess the score of a randomly drawn participant from a previous session. 
All they knew about the other participant was their gender identity and whether they were US nationals or not.
For each guess they had three score options: Low-Score (5 or fewer correct answers), Mid-Score (between 6 and 15 correct answers), 
High-Score (16 or more). Each of the score categories correspond to $\theta_L$, $\theta_M$, and $\theta_H$ respectively. 
They were also asked to say how confident they felt about their choices. They had 4 possible answers: "it was a random guess", 
"there is another equally likely score", "I am pretty sure", "I am completely sure". 
These 4 answers are mapped to priors that place probabilities 
.33, .50, .75, and 1 to the chosen type. The remaining probability is split equally among the other two types. 
Questions in part 2
were not incentivized, but subjects were told that providing an accurate answer would increase their chances of earning more money in 
the last part of the experiment.

The purpose of part 2 is to classify subjects into overconfident, underconfident and correctly specified. If a subject 
guesses their score to be in a higher (lower) category than their true score, they are overconfident (underconfident); if they 
guess their score to be in the same category as their true score, they are correctly specified. 
This classification is done for each of the 6 topics separately.

Finally, in part 3 subjects completed a belief updating task for each of the quizzes. Before starting the task they were reminded of their guess
for the score. In the ego treatment they were reminded of their guess about themselves and in the stereotype treatment they were reminded
of their guess about the other participant. In the stereotype treatment, they were also reminded of the characteristics of the other participant.

For one topic at a time and in random order,
they were presented with the three gambles from the example above, and were asked to choose one of them. 
The probability of success was 
determined by their own score in the ego-relevant condition, and by the score of the other participant in the stereotype condition. 
Subjects had access to the three probability
tables in the printout of the instructions at all times and the meaning of each cell was explained in detail. 

In the interphase, they had to choose which of the 3 tables they wanted to see before entering their choice in it. This was done as 
an alternative to a belief elicitation in each round. I take their choice of table to be a signal for their beliefs about
the underlying type. I chose not to elicit the beliefs at each round to stay true to the forces in framework 1.

Once they have entered their choice, they observe a sample of 10 outcomes from the gamble they chose. After observing the outcomes, 
they returned to the choice screen and entered a new choice. In the choice screen 
subjects had access to the entire history of gambles and outcomes for that task as well as a summary of the outcomes so far. 
Once they entered 11 gambles (and observed 110 outcomes), they moved on to 
the next topic and repeated the same procedure. They all did this for all 6 topics. 

At the end of the experiment, one of the 6 topics was randomly selected to determine the payment. They earned $\$0.20$ for each 
correct answer in the quiz, and for each success in part 3.

Randomness is controlled throughout the experiment and sessions by setting a seed at the beggining of the
first session. The seed was drawn at random and remained fixed for all sessions. 
By doing this I ensure that any two subjects who have the same type and face the same exogenous rate will observe
the same outcomes and thus, if they use the same updating procedure, they should be choosing the same gambles. 
This design feature allows me to identify differences in updating procedures across subjects. 

# Predictions

Because the different models apply only under certain conditions, I split the predictions into two groups; the dogmatic and the 
switcher constitute the first group, and the bayesian and self-serving updater are the second group. the predictions of group 1
apply only in cases where $(\theta, \omega)$ are such that there is a self-defeating equilibrium. This is because whenever 
there is no self defeating equilibria, the dogmatic model does not apply. On the other hand, the theories of group 2 apply to all
possible combinations of the parameters. Hence, these theories can be tested and differentiated within a broader set of 
observations. 

It is also important to note that since the predictions of the model are asymptotic and probabilistic, the predictions that are made 
for the particular example that was taken to the lab depend strongly on the realized sequence of signals. Because the randomness has 
been controlled for and kept fixed across sessions, for every parametrization $(\theta, \omega)$ there is a unique optimal path 
for each of the models. I present the predictions made by this particular realization of the seed.\footnote{The seed that was drawn 
at the beginning of session 1 was 3452. The same seed was used for all sessions. It is used both for drawing $\omega$ for each of 
the tasks in the experiment, as well as for drawing the outcomes from the gambles.}

## Dogmatic and Switcher

The cases in which we can apply these two theories are the five self-defeating equilibria from the data generating process in the 
example. From these and with the seed that was drawn, the models make different predictions in 4 cases. \footnote{the fifth 
self-defeating equilibrum is such that the likelihood ratio does not go above 1 and thus the switcher is indistinguishable from 
the dogmatic agent.}

## Bayesian and self-attribution updater



# Analysis




# Conclusion