% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  12pt,
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Learning from Data Through Models},
  pdfauthor={Jimena Galindo},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=2.5cm]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}

\setlength{\parindent}{25pt}
\setlength{\parskip}{0pt}
\setlength{\skip\footins}{0.25cm} % margin before footnotes
\definecolor{myblue}{RGB}{31, 61, 122}
\definecolor{mypink}{RGB}{204, 0, 82}
\newtheorem{assumption}{Assumption}
\newtheorem{proposition}{Proposition} % Use the 'proposition' counter for numbering

\newcommand{\EE}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\PP}[1]{\mathbb{P}\left(#1\right)}
\newcommand{\CP}[2]{\mathbb{P}\left(#1 \,| \, #2 \right)}
\newcommand{\CE}[2]{\mathbb{E}\left[#1\,|\,#2 \right]}
\usepackage[bottom]{footmisc}
\usepackage[doublespacing]{setspace}
\usepackage[normal]{caption}
\usepackage{dsfont}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{hyperref}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}

\title{Learning from Data Through Models}
\author{Jimena Galindo}
\date{September 03, 2023}

\begin{document}
\maketitle
\begin{abstract}
TBW
\end{abstract}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\hypertarget{framework}{%
\section{Framework}\label{framework}}

REWRITE IN GENERAL TERMS WITH THE PROBABILITY BEING GIVEN BY A FUNCTION
THAT SATIEFIES ALL THE PROPERTIES IN BA SO THAT I CAN REFER TO HER
RESULTS. THEN EXPAND ON THE EXAMPLE FOR THE LAB AND SAY WHY IT CAPTURES
ALL THE FORCES WE WANT.

An agent is of type \(\theta \in \{\theta_L, \theta_M, \theta_H\}\) with
\(\theta_H > \theta_M > \theta_L\) and they face an unknown exogenous
state \(\omega \in \{\omega_L, \omega_M, \omega_H\}\) with
\(\omega_H>\omega_M>\omega_L\). Each of the values of \(\omega\) is
realized with equal probability. The agent knows the distribution of
\(\omega\) but not its value. They also hold some prior belief about
\(\theta\) which is potentially
misspecified.\footnote{The particular types of misspecification
that are allowed are discussed below.} The agent chooses a binary gamble
\(e in \{e_L, e_M, e_H\}\) and observes whether the gamble is a success
or a failure. ' If it is a success, the agent gets a payoff of \(1\) and
if it is a failure, the agent gets a payoff of \(0\). The probability of
success is increasing in both \(\theta\) and \(\omega\) and is fully
described in the following table:

\begin{tabular}{ c|c|c|c|}
  
  \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\omega_H$} & \multicolumn{1}{c}{$\omega_M$} & \multicolumn{1}{c}{$\omega_L$}\\
  \cline{2-4}
  $e_H$ & 50 & 20 & 2 \\
  \cline{2-4}
  $e_M$ & 45 & 30 & 7 \\
  \cline{2-4}
  $e_L$ & 40 & 25 & 20 \\
  \cline{2-4}
  \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\theta_L$} & \multicolumn{1}{c}{}\\
\end{tabular}
\hspace{.3cm} 
\begin{tabular}{ c|c|c|c|}
  
  \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\omega_H$} & \multicolumn{1}{c}{$\omega_M$} & \multicolumn{1}{c}{$\omega_L$}\\
  \cline{2-4}
  $e_H$ & 80 & 50 & 5 \\
  \cline{2-4}
  $e_M$ & 69 & 65 & 30 \\
  \cline{2-4}
  $e_L$ & 65 & 45 & 40 \\
  \cline{2-4}
  \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\theta_M$} & \multicolumn{1}{c}{}\\
\end{tabular}
\hspace{.3cm} 
\begin{tabular}{ c|c|c|c|}
  
  \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\omega_H$} & \multicolumn{1}{c}{$\omega_M$} & \multicolumn{1}{c}{$\omega_L$}\\
  \cline{2-4}
  $e_H$ & 98 & 65 & 25 \\
  \cline{2-4}
  $e_M$ & 80 & 69 & 35 \\
  \cline{2-4}
  $e_L$ & 75 & 55 & 45 \\
  \cline{2-4}
  \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\theta_H$} & \multicolumn{1}{c}{}\\
\end{tabular}

Conditional on a type, the agent's flow payoff is maximized by choosing
the gamble that matches the state. For example, if the value of
\(\omega\) is \(\omega_H\), the agent's flow payoff is maximized by
choosing \(e_H\) and if the state is \(\omega_L\) the flow payoff is
maximized by choosing gamble \(e_L\). The agent myopically chooses
gambles every period to maximize the flow payoff for \(T<\infty\)
periods, and the agent's payoff is the sum of the payoffs from each
period.

After observing the outcome of each gamble, the agent updates their
beliefs of both theta and omega in one of 4 different ways: As a
\emph{dogmatic} as in \citet{Heidhues2018} ; as a \emph{switcher} as in
\citet{Ba2023} ; as a \emph{self-attribution} updater as in
\citet{Coutts2020}; or as a fully Bayesian agent. Each of these updating
rules is described in detail below.

Notice that both \(\theta\) and \(\omega\) can be identified from the
outcomes if enough variation in the effort choices exists. This can be
seen by confirming that no two columns are the same. Therefore, the
agent can learn both their type and the state by observing enough
outcomes of the gambles for different effort choices.

\hypertarget{the-bayesian-benchmark}{%
\subsection{The Bayesian Benchmark}\label{the-bayesian-benchmark}}

A Bayesian agent simultaneously updates their beliefs about \(\theta\)
and \(\omega\) by using Bayes' rule. The posterior odds at period t
about \(\theta\) after observing an outcome are given by:
\begin{equation}
\frac{p_{t}[\theta_H|\text{outcome}]}{p_{t}[\theta_M|\text{outcome}]} = 
      \frac{p[\text{outcome}|\theta_H]p_{t-1}[\theta_H]}{p[\text{outcome}|\theta_M]p_{t-1}[\theta_M]}
\end{equation} and \begin{equation}
\frac{p_{t}[\theta_M|\text{outcome}]}{p_{t}[\theta_L|\text{outcome}]} = 
      \frac{p[\text{outcome}|\theta_M]p_{t-1}[\theta_M]}{p[\text{outcome}|\theta_L]p_{t-1}[\theta_L]}
\end{equation}

Where \(p_{t-1}\) is the prior at period \(t\) and
\(p[\text{outcome}|\theta] = \sum_{\omega} p[\text{outcome}|\theta, \omega, e]p_{t-1}(\omega)\)
is the probability of observing the outcome given the agent's type and
the effort chosen. The update is symmetric for \(\omega\).

Bayesian agents always choose the effort level that maximizes their flow
payoff by taking expectations over their prior beliefs about \(\theta\)
and \(\omega\). Since agents are myopic, even though all the parameters
could be identified with enough variation in choices, a fully Bayesian
agent might not learn their true type. This happens because they do not
internalize the tradeoff between flow payoff and learning and thus might
not experiment enough to learn their type. An alternative to this
approach is given by \citet{Hestermann2021} and is discussed with the
results.

\hypertarget{a-biased-agent}{%
\subsection{A Biased Agent}\label{a-biased-agent}}

An agent that updates their beliefs with self-attribution bias will
update their beliefs about the state \(\omega\) and their type
\(\theta\) by over-attributing successes to a high value of \(\theta\)
and under-estimating the role of higher \(\omega\). Similarly, they will
attribute failure to a low state more than an unbiased agent would. To
model the self-serving attribution bias, I take the approach of
\citet{benjamin2019}, where a generalization of the Bayes rule above
gives the update.

\begin{equation}
\frac{p_{t}[\theta_H|\text{outcome}]}{p_{t}[\theta_M|\text{outcome}]} = 
      \left(\frac{p[\text{outcome}|\theta_H]}{p[\text{outcome}|\theta_M]}\right)^{c_s^{\theta}\mathbb{I}\{\text{success}\}+c_f^{\theta}\mathbb{I}\{\text{failure}\}}\frac{p_{t-1}[\theta_H]}{p_{t-1}[\theta_M]}
\end{equation} and \begin{equation}
\frac{p_{t}[\theta_M|\text{outcome}]}{p_{t}[\theta_L|\text{outcome}]} = 
      \left(\frac{p[\text{outcome}|\theta_M]}{p[\text{outcome}|\theta_L]}\right)^{c_s^{\theta}\mathbb{I}\{\text{success}\}+c_f^{\theta}\mathbb{I}\{\text{failure}\}}\frac{p_{t-1}[\theta_M]}{p_{t-1}[\theta_L]}
\end{equation}

\(c_s^{\theta}\) and \(c_f^{\theta}\) are the self-serving attribution
bias parameters for the agent's type \(\theta\). If
\(c_s^{\theta} = c_f^{\theta} = 1\), the agent is unbiased and the
update is the same as the Bayesian update. On the other hand, if
\(c_s^{\theta} > c_f{\theta}\) the agent over-attributes success to
their type and under-attributes failure to their type\footnote{
  notice that the values of $c_s^{\theta}$ and $c_f^{\theta}$ are not restricted to be greater than 1. If they are both equal to 
  each other but less (more) than one, then the bias is simply underinference (overinference).}.

The update for \(\omega\) is analogous but with \(c_f^{\omega}\) and
\(c_s^{\omega}\) instead of \(c_f^{\theta}\) and \(c_s^{\theta}\) and
the bias is present whenever \(c_f^{\omega} > c_s^{\omega}\). That is,
the agent over-attributes failure to a low state relative the higher
states and under-attributes success to a low state relative to the
higher states.

\hypertarget{the-dogmatic-agent}{%
\subsection{The Dogmatic Agent}\label{the-dogmatic-agent}}

A dogmatic agent does not update their beliefs about \(\theta\);
instead, they hold a degenerate belief that places probability one on
being a particular type, \(\hat{\theta}\). For instance, the dogmatic
agent might be of type \(\hat{\theta} = \theta_M\) but holds a belief
that places probability one on being of type \(\theta_H\). In this case,
no matter how much information he gathers against being of type
\(\theta_H\), he will not update his beliefs. Any discrepancies between
the observed outcomes are incorporated using the Bayes rule to update
their beliefs about \(\omega\). This results in the following posterior
odds:

\begin{equation}
\frac{p_{t}[\omega_H|\text{outcome}]}{p_{t}[\omega_M|\text{outcome}]} = 
      \frac{p[\text{outcome}|\omega_H, \hat{\theta}, e]p_{t-1}[\omega_H]}{p[\text{outcome}|\omega_M, \hat{\theta}, e]p_{t-1}[\omega_M]}
\end{equation} and \begin{equation}
\frac{p_{t}[\omega_M|\text{outcome}]}{p_{t}[\omega_L|\text{outcome}]} = 
      \frac{p[\text{outcome}|\omega_M, \hat{\theta}, e]p_{t-1}[\omega_M]}{p[\text{outcome}|\omega_L, \hat{\theta}, e]p_{t-1}[\omega_L]}
\end{equation}

A crucial difference between the dogmatic agent and the Bayesian agent
is that the dogmatic agent does not aggregate across types when updating
their beliefs about \(\omega\). This means the dogmatic agent will never
learn their true type if they are misspecified.

\citet{Heidhues2018} show that in a setting such as ours, a dogmatic
modeler will inevitably fall into a self-confirming equilibrium where
the outcomes they observe reinforce their belief on \(\omega\) in such a
way that as \(t\to\infty\) the agent will be certain that the state is
some \(\omega^*\) consistent with their believed type and the observed
data.

\hypertarget{the-switcher}{%
\subsection{The Switcher}\label{the-switcher}}

An agent is a \emph{switcher} if they behave as a dogmatic but is
willing to entertain the possibility that they are of a different type.
In particular, when they start off as a misspecified dogmatic, they are
willing to switch to a different dogmatic belief if the data is
convincing enough.

In order to abandon their initial dogmatic belief, the agent needs to
observe a sequence of outcomes that are sufficiently unlikely to have
happened if they were of the type they initially believed. They do so by
keeping track of the likelihood that each of the possible types
generated the data. If the likelihood ratio is sufficiently large, the
agent will switch to the alternative and behave as if they are dogmatic
about the new type.

In particular, for an agent that starts off with a dogmatic belief that
they are of type \(\hat{\theta}\) but is willing to consider the
alternative explanation that they are of type \(\tilde{\theta}\), the
agent will switch to the alternative if:

\[\frac{p[h^t|\tilde{\theta}]}{p[h^t|\hat{\theta}]} > \alpha\]

Where \(h^t\) is the history of outcomes up to time \(t\) and
\(\alpha \geq 1\) is a threshold that determines how convincing the data
needs to be for the agent to change their belief. Notice that if
\(\alpha \to \infty\), we get the dogmatic agent. In this sense, the
switcher is a generalization of the dogmatic type, just as the
self-attribution agent is a generalization of the Bayesian agent.

By allowing the agent to keep track of the likelihoods and switching to
an alternative type, the switcher can avoid the self-confirming
equilibrium the dogmatic agent falls into. However, if the prior belief
on \(theta\) is sufficiently tight around a self-confirming equilibrium,
the switcher might look identical to the dogmatic even in a case where
\(\alpha\) is not too large.

\hypertarget{experimental-design}{%
\section{Experimental Design}\label{experimental-design}}

\hypertarget{analysis}{%
\section{Analysis}\label{analysis}}

\renewcommand\refname{Conclusion}
  \bibliography{references.bib}

\end{document}
