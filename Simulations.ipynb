{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "738b7f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eda7e01",
   "metadata": {},
   "source": [
    "## Setting\n",
    "An agent can be one of three types: $\\theta_L = 0, \\theta_M= 1$ or $\\theta_H= 2$. They choose an effort level $e\\in \\{1, 2, 3\\}$ and produce an output $y=A+B[(\\theta + e)\\omega - \\frac{e^2}{2}]$ where $\\omega$ is an exogenous parameter that can take the values $\\omega_L = 1, \\omega_M = 2$ or $\\omega_H =3$ each with $1/3$ probability. I call $\\omega$ the exchange rate. A and B are non-negative parameters.\n",
    "\n",
    "Instead of observing the output y, the agent observes only a success or a failure. The probability that they get a success is given by $p[success|\\theta, \\omega, e]=\\frac{y}{K+y}$ where $K$ is a non-negative constant.\n",
    "A, B and K must be chosen to satisfy some conditions: y is non-negative, p and (1-p) are log-submodular ($p_{\\theta, \\omega}p\\leq p_\\theta p\\omega$ and $p_{\\theta, \\omega}(1-p)\\leq p_\\theta p_\\omega$). And I also need to generate probabilities of success that are distinct enough across types and exchange rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfcb6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# three possible values for the agent's type\n",
    "Theta_L = 0\n",
    "Theta_M = .3\n",
    "Theta_H = .5\n",
    "# three possible values for the exogenous parameter\n",
    "w_l = 1\n",
    "w_m = 3\n",
    "w_h = 5\n",
    "\n",
    "w = [w_l, w_m, w_h]\n",
    "\n",
    "# the possible effort choices have to correspond to the possible exchange rates w because those values max output\n",
    "effort = w\n",
    "\n",
    "# Three extra parameters for the output function and the probability transformation\n",
    "A = 2\n",
    "B = 1/4\n",
    "K = 1\n",
    "param = [A, B, K]\n",
    "\n",
    "#the prior is uniform over omega\n",
    "pi_0 = [1/3, 1/3, 1/3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa06c383",
   "metadata": {},
   "source": [
    "A function that generates the output as in Heidhues, Koszegi and Strack and transforms it into a probability of success. The transformation ensures that all the important properties are preserved. The probability should be log-submodular as in Hestermann and Le Yaouanq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0083006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(pars, e, o, w):\n",
    "    out = pars[0]+pars[1]*((o+e)*w-(e**2)/2)\n",
    "    prob_s = out/(out+pars[2])\n",
    "    return prob_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e52917b",
   "metadata": {},
   "source": [
    "With the function $p$ I can generate the probability of success for all possible combinations of $(\\theta, \\omega, e)$. Each matrix represents a type $\\theta$, each column represents an exchang rate in ascending order and the rows are effort also in ascending order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "992577cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low theta\n",
      "[[0.68       0.72413793 0.75757576]\n",
      " [0.61904762 0.75757576 0.82222222]\n",
      " [0.11111111 0.72413793 0.83673469]]\n",
      "medium theta\n",
      "[[0.6875     0.74025974 0.77777778]\n",
      " [0.62962963 0.77011494 0.83333333]\n",
      " [0.16666667 0.74025974 0.84615385]]\n",
      "high theta\n",
      "[[0.69230769 0.75       0.78947368]\n",
      " [0.63636364 0.77777778 0.84      ]\n",
      " [0.2        0.75       0.85185185]]\n"
     ]
    }
   ],
   "source": [
    "# calculate the true probability matrices\n",
    "Mat_L = []\n",
    "Mat_M = []\n",
    "Mat_H = []\n",
    "\n",
    "for e in effort:\n",
    "    Mat_L.append([p(param, e, Theta_L, w[0]), p(param, e, Theta_L, w[1]), p(param, e, Theta_L, w[2])])\n",
    "    Mat_M.append([p(param, e, Theta_M, w[0]), p(param, e, Theta_M, w[1]), p(param, e, Theta_M, w[2])])\n",
    "    Mat_H.append([p(param, e, Theta_H, w[0]), p(param, e, Theta_H, w[1]), p(param, e, Theta_H, w[2])])\n",
    "\n",
    "Mat_L = np.array(Mat_L)\n",
    "Mat_M = np.array(Mat_M)\n",
    "Mat_H = np.array(Mat_H)\n",
    "\n",
    "M0 = [Mat_L, Mat_M, Mat_H]\n",
    "\n",
    "print('low theta')\n",
    "print(Mat_L)\n",
    "print('medium theta')\n",
    "print(Mat_M)\n",
    "print('high theta')\n",
    "print(Mat_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f2dd46",
   "metadata": {},
   "source": [
    "Here are matrices that are not computed with the formula above. I modified the values to get larger differences from one choice to another without altering the way in which the values are ordered. I did it so that the lowest value is 0% and the largest is 90%. While having at least a 5% point jump between elements of the same matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba29f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "ML = np.array([[.30, .40, .55], [.20, .55, .65], [0, .40, .75]])\n",
    "MM = np.array([[.35, .45, .60], [.25, .60, .70], [.05, .45, .80]])\n",
    "MH = np.array([[.40, .50, .65], [.30, .65, .80], [.10, .50, .95]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1eed13",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "I will define the output function separately just in case we need it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ab8ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def y(pars, e, o, w):\n",
    "    out = pars[0]+pars[1]*((o+e)*w-(e**2)/2)\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73acc247",
   "metadata": {},
   "source": [
    "With the matrices now I can compute the expected utility for each of the effort levels. This way I know which effort each agent would choose depending on the type that they believe that they are. (I will start with a type with a degenerate belief and then extend to the bayesian with self serving attribution later on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a9b1558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that computes the expected probability of success under each effort choice\n",
    "# given the belief about the omegas. The expected value muyt be computed separately for each type.\n",
    "def Eu(prior, type_belief, M):\n",
    "    # type belief should be 0, 1, or 2\n",
    "    # the prior is over omegas. \n",
    "    # M is a vector of the three probability matrices (L, M, H)\n",
    "    Eu = np.dot(M[type_belief], prior)\n",
    "    return Eu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1734a4",
   "metadata": {},
   "source": [
    "For whatever theta the agent thinks that is their type, they will choose the effort level that maximizes their payoff given their prior. The next function finds the index of the effort level that thy will choose. It takes as an argument the vector of expected utilities for the believed type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c89131ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that finds the effort level given the prior over omega\n",
    "def choice(Eu_believed):\n",
    "    e_index = np.argmax(Eu_believed)\n",
    "    return e_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fc75bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function for the bayes update on omega\n",
    "def bayesW(p0, e_index, o, M, believed_type):\n",
    "    matrix = M[believed_type]\n",
    "    row = matrix[e_index, :]\n",
    "    if o == 1:\n",
    "        p1 = np.diag(np.diagflat(p0)@np.diagflat(row))/np.sum(np.diag(np.diagflat(p0)@np.diagflat(row)))\n",
    "    else:\n",
    "        p1 = np.diag(np.diagflat(p0)@np.diagflat(1-row))/np.sum(np.diag(np.diagflat(p0)@np.diagflat(1-row)))\n",
    "        \n",
    "    return p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8a79e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that updates omega and theta with a self serving attribution bias\n",
    "def ss_update(gs_theta, gs_omega, gf_theta, gf_omega, p_omega, p_theta, e_index, o, M):\n",
    "\n",
    "    matrix = np.array([M[0][e_index, :], M[2][e_index, :], M[2][e_index, :]])\n",
    "    \n",
    "    # g_theta and g_omega should have 3 elements: the distortion parameters for L, M and H. if all are equal to 1 \n",
    "    # is the bayesian baseline.\n",
    "    \n",
    "    p0s_omega = np.diag(np.diagflat(p_omega) @ np.diagflat(gs_omega))\n",
    "    p0s_theta = np.diag(np.diagflat(p_theta) @ np.diagflat(gs_theta))\n",
    "    \n",
    "    p0f_omega = np.diag(np.diagflat(p_omega) @ np.diagflat(gf_omega))\n",
    "    p0f_theta = np.diag(np.diagflat(p_theta) @ np.diagflat(gf_theta))\n",
    "\n",
    "    if o == 1:\n",
    "        num_omega = np.diag( np.diagflat(p0s_omega) @ np.diagflat(np.array(p0s_theta) @ matrix ) )\n",
    "        num_theta = np.diag( np.diagflat(p0s_theta) @ np.diagflat(matrix @ np.array(p0s_omega) ) )\n",
    "        denom_o = np.sum(num_omega)\n",
    "        denom_th = np.sum(num_theta)\n",
    "        p1_omega = num_omega/denom_o\n",
    "        p1_theta = num_theta/denom_th\n",
    "    else:\n",
    "\n",
    "        num_omega = np.diag( np.diagflat(p0f_omega) @ np.diagflat(np.array(p0f_theta) @ (1-matrix) ) )\n",
    "        num_theta = np.diag( np.diagflat(p0f_theta) @ np.diagflat((1 - matrix ) @ np.array(p0f_omega) ) )\n",
    "        denom_o = np.sum(num_omega)\n",
    "        denom_th = np.sum(num_theta)\n",
    "        p1_omega = num_omega/denom_o\n",
    "        p1_theta = num_theta/denom_th\n",
    "        \n",
    "    return p1_theta, p1_omega"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8895850",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "For T periods and N agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676f194a",
   "metadata": {},
   "source": [
    "A function that takes in the true type $\\theta$ and the true exogenous parameter $\\omega$ and simulates data for the three possible models ($\\theta_L$, $\\theta_M$, $\\theta_H$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07e0c591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_data(w_true, type_true, pi_0, T, N, M):\n",
    "    # w_true is the exogenous parameter/exchange rate\n",
    "    # type true is the true value of theta. \n",
    "    # prior is the prior over the three possible values of w \n",
    "    # T is the number of periods\n",
    "    # N is the number of agents it will be simulated for\n",
    "    #M is the matrices of probabilities under all three models\n",
    "\n",
    "    # set the alternative types (misspecified models)\n",
    "    type_L = 0\n",
    "    type_M = 1\n",
    "    type_H = 2\n",
    "\n",
    "    pop_ew_L = []\n",
    "    pop_ew_M = []\n",
    "    pop_ew_H = []\n",
    "\n",
    "    pop_choices_L = []\n",
    "    pop_choices_M = []\n",
    "    pop_choices_H = []\n",
    "\n",
    "    pop_lr_L = []\n",
    "    pop_lr_M = []\n",
    "    pop_lr_H = []\n",
    "\n",
    "    for n in range(N):\n",
    "        # save the outcome history\n",
    "        out_L = []\n",
    "        out_M = []\n",
    "        out_H = []\n",
    "\n",
    "        # save the posterior history (bayes on $\\omega$ with fixed $\\theta$)\n",
    "        pw_L = [pi_0]\n",
    "        pw_M = [pi_0]\n",
    "        pw_H = [pi_0] \n",
    "\n",
    "        #save the vectors of probabilities under the believed model and under the true model (to compute the likelihhod ratio)\n",
    "        p_L_model = []\n",
    "        p_M_model = []\n",
    "        p_H_model = []\n",
    "\n",
    "        p_L_true = []\n",
    "        p_M_true = []\n",
    "        p_H_true = []\n",
    "\n",
    "        # save the likelihood\n",
    "        likelihood_L = [1]\n",
    "        likelihood_M = [1]\n",
    "        likelihood_H =[1]\n",
    "\n",
    "        # save the effort history\n",
    "        e_L = []\n",
    "        e_M = []\n",
    "        e_H = []\n",
    "\n",
    "        # the starting beliefs for each type\n",
    "        p0_L = pi_0\n",
    "        p0_M = pi_0\n",
    "        p0_H = pi_0\n",
    "\n",
    "        # the expected value of $\\omega$\n",
    "        Ew_L = [np.dot([0, 1, 2], pi_0)]\n",
    "        Ew_M = [np.dot([0, 1, 2], pi_0)]\n",
    "        Ew_H = [np.dot([0, 1, 2], pi_0)]\n",
    "        \n",
    "        # simulate a single agent for T periods\n",
    "        for i in range(T):\n",
    "            #compute the expected success rate from every choice\n",
    "            Eu_H = Eu(p0_H, 2, M)\n",
    "            Eu_M = Eu(p0_M, 1, M)\n",
    "            Eu_L = Eu(p0_L, 0, M)\n",
    "\n",
    "            # save the effort that maximizes their success rate in the effort history vector\n",
    "            choice_H = choice( Eu_H ) \n",
    "            choice_M = choice( Eu_M ) \n",
    "            choice_L = choice( Eu_L ) \n",
    "\n",
    "            # Draw a realization using the probability under the true type, for a given true $\\omega$5\n",
    "            outcome_H = np.random.binomial(1, M[type_true][choice_H, w_true], size=None)\n",
    "            outcome_M = np.random.binomial(1, M[type_true][choice_M, w_true], size=None)\n",
    "            outcome_L = np.random.binomial(1, M[type_true][choice_L, w_true], size=None)\n",
    "\n",
    "            # update the belief on omega given the outcome\n",
    "            p1_H = bayesW(p0_H, choice_H, outcome_H, M, type_H)\n",
    "            p1_L = bayesW(p0_L, choice_L, outcome_L, M, type_L)\n",
    "            p1_M = bayesW(p0_M, choice_M, outcome_M, M, type_M )\n",
    "\n",
    "            # compute the likelihood ratio of the true type over their believed type.\n",
    "            # First save all the probabilities of the realized types\n",
    "            if outcome_H == 1:\n",
    "                p_H_model.append(M[type_H][choice_H, :])\n",
    "                p_H_true.append(M[type_true][choice_H, :])\n",
    "\n",
    "            else:\n",
    "                p_H_model.append(1 - M[type_H][choice_H, :])\n",
    "                p_H_true.append(1 - M[type_true][choice_H, :])\n",
    "\n",
    "            if outcome_L == 1:\n",
    "                p_L_model.append(M[type_L][choice_L, :])\n",
    "                p_L_true.append(M[type_true][choice_L, :])\n",
    "            else:\n",
    "                p_L_model.append(1 - M[type_L][choice_L, :])\n",
    "                p_L_true.append(1 - M[type_true][choice_L, :])\n",
    "                \n",
    "            if outcome_M == 1:\n",
    "                p_M_model.append(M[type_M][choice_M, :])\n",
    "                p_M_true.append(M[type_true][choice_M, :])\n",
    "            else:\n",
    "                p_M_model.append(1 - M[type_M][choice_M, :])\n",
    "                p_M_true.append(1 - M[type_true][choice_M, :])\n",
    "                \n",
    "            \n",
    "            # compute the likelihoods for each model and alternative (the models are the believed types and the alternative is always the truth)\n",
    "            likelihood_H.append(np.dot(np.prod(np.array(p_H_true), axis=0), pi_0)/np.dot(np.prod(np.array(p_H_model), axis=0), pi_0))\n",
    "            likelihood_L.append(np.dot(np.prod(np.array(p_L_true), axis=0), pi_0)/np.dot(np.prod(np.array(p_L_model), axis=0), pi_0))\n",
    "            likelihood_M.append(np.dot(np.prod(np.array(p_M_true), axis=0), pi_0)/np.dot(np.prod(np.array(p_M_model), axis=0), pi_0))\n",
    "            \n",
    "\n",
    "            # their expectation of $\\omega$ each period\n",
    "            Ew_H.append(np.dot([0, 1, 2], p1_H))\n",
    "            Ew_L.append(np.dot([0, 1, 2], p1_L))\n",
    "            Ew_M.append(np.dot([0, 1, 2], p1_M))\n",
    "\n",
    "            # add to the history vector of outcomes\n",
    "            out_H.append( outcome_H )\n",
    "            out_M.append( outcome_M )\n",
    "            out_L.append( outcome_L )\n",
    "\n",
    "            # add to the history of beliefs and update the prior\n",
    "            # save the effort that maximizes their success rate in the effort history vector\n",
    "            e_H.append( choice_H )\n",
    "            e_M.append( choice_M )\n",
    "            e_L.append( choice_L )\n",
    "\n",
    "            pw_H.append( p1_H )\n",
    "            pw_M.append( p1_M )\n",
    "            pw_L.append( p1_L )\n",
    "\n",
    "            p0_H = p1_H\n",
    "            p0_M = p1_M\n",
    "            p0_L = p1_L\n",
    "\n",
    "        pop_ew_H.append(Ew_H)\n",
    "        pop_ew_L.append(Ew_L)\n",
    "        pop_ew_M.append(Ew_M)\n",
    "\n",
    "        pop_choices_H.append(e_H)\n",
    "        pop_choices_L.append(e_L)\n",
    "        pop_choices_M.append(e_M)\n",
    "\n",
    "        pop_lr_H.append( likelihood_H )\n",
    "        pop_lr_L.append( likelihood_L )\n",
    "        pop_lr_M.append( likelihood_M )\n",
    "\n",
    "    # turn each table into a data frame\n",
    "    df_ew_H = pd.DataFrame(pop_ew_H)\n",
    "    df_ew_L = pd.DataFrame(pop_ew_L)\n",
    "    df_ew_M = pd.DataFrame(pop_ew_M)\n",
    "\n",
    "    df_choices_H = pd.DataFrame(pop_choices_H)\n",
    "    df_choices_L = pd.DataFrame(pop_choices_L)\n",
    "    df_choices_M = pd.DataFrame(pop_choices_M)\n",
    "\n",
    "    df_lr_H = pd.DataFrame(pop_lr_H)\n",
    "    df_lr_L = pd.DataFrame(pop_lr_L)\n",
    "    df_lr_M = pd.DataFrame(pop_lr_M)\n",
    "\n",
    "    df_ew_H.columns = ['ew_H'+str(t) for t in range(T+1)]\n",
    "    df_ew_L.columns = ['ew_L'+str(t) for t in range(T+1)]\n",
    "    df_ew_M.columns = ['ew_M'+str(t) for t in range(T+1)]\n",
    "\n",
    "    df_choices_H.columns = ['choice_H'+str(t) for t in range(T)]\n",
    "    df_choices_L.columns = ['choice_L'+str(t) for t in range(T)]\n",
    "    df_choices_M.columns = ['choice_M'+str(t) for t in range(T)]\n",
    "\n",
    "    df_lr_H.columns = ['lr_H'+str(t) for t in range(T+1)]\n",
    "    df_lr_L.columns = ['lr_L'+str(t) for t in range(T+1)]\n",
    "    df_lr_M.columns = ['lr_M'+str(t) for t in range(T+1)]\n",
    "\n",
    "    # go from multiple wide data frame to a single long one for $\\omega_L$\n",
    "    df = pd.concat([df_ew_H, \n",
    "                   df_ew_L, \n",
    "                   df_ew_M, \n",
    "                   df_choices_H, \n",
    "                   df_choices_L, \n",
    "                   df_choices_M, \n",
    "                   df_lr_H, \n",
    "                   df_lr_L,\n",
    "                   df_lr_M], axis=1)\n",
    "\n",
    "    df[\"id\"] = df.index\n",
    "\n",
    "    df = pd.wide_to_long(df, [\"ew_H\", \n",
    "                                  \"ew_L\", \n",
    "                                  \"ew_M\", \n",
    "                                  \"choice_H\", \n",
    "                                  \"choice_L\", \n",
    "                                  \"choice_M\", \n",
    "                                  \"lr_H\",\n",
    "                                  \"lr_L\", \n",
    "                                  \"lr_M\"], \n",
    "                             i=\"id\", j=\"year\")\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ffce1",
   "metadata": {},
   "source": [
    "## Function to simulate the likelihood ratio test choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a0a1da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_indiv(w_true, type_true, type_belief, T, M, pi_0, alpha):\n",
    "    # save the outcome history\n",
    "    out = []\n",
    "    types = [type_belief]\n",
    "\n",
    "        # save the posterior history (bayes on $\\omega$ with fixed $\\theta$)\n",
    "    pw = [pi_0] \n",
    "\n",
    "        #save the vectors of probabilities under the believed model and under the true model (to compute the likelihhod ratio)\n",
    "    p_model = []\n",
    "\n",
    "    p_true = []\n",
    "\n",
    "        # save the likelihood\n",
    "    likelihood = [1]\n",
    "\n",
    "        # save the effort history\n",
    "    e = []\n",
    "\n",
    "        # the starting beliefs for each type\n",
    "    p0 = pi_0\n",
    "\n",
    "        # the expected value of $\\omega$\n",
    "    Ew = [np.dot([0, 1, 2], pi_0)]\n",
    "        \n",
    "        # simulate a single agent for T periods\n",
    "    for i in range(T):\n",
    "            #compute the expected success rate from every choice\n",
    "        u = Eu(p0, type_belief, M)\n",
    "\n",
    "            # save the effort that maximizes their success rate in the effort history vector\n",
    "        chose = choice( u )  \n",
    "\n",
    "            # Draw a realization using the probability under the true type, for a given true $\\omega$5\n",
    "        outcome = np.random.binomial(1, M[type_true][chose, w_true], size=None)\n",
    "\n",
    "            # update the belief on omega given the outcome\n",
    "        p1 = bayesW(p0, chose, outcome, M, type_belief)\n",
    "\n",
    "        # compute the likelihood ratio of the true type over their believed type.\n",
    "        # First save all the probabilities of the realized types\n",
    "        if outcome == 1:\n",
    "            p_model.append(M[type_belief][chose, :])\n",
    "            p_true.append(M[type_true][chose, :])\n",
    "\n",
    "        else:\n",
    "            p_model.append(1 - M[type_belief][chose, :])\n",
    "            p_true.append(1 - M[type_true][chose, :])\n",
    "                \n",
    "        LR = np.dot(np.prod(np.array(p_true), axis=0), pi_0)/np.dot(np.prod(np.array(p_model), axis=0), pi_0)\n",
    "            \n",
    "        if LR > alpha:\n",
    "            type_belief = type_true\n",
    "            types.append(type_belief)\n",
    "        else:\n",
    "            type_belief = type_belief\n",
    "            types.append(type_belief)\n",
    "            \n",
    "            # compute the likelihoods for each model and alternative (the models are the believed types and the alternative is always the truth)\n",
    "        likelihood.append(LR)  \n",
    "\n",
    "            # their expectation of $\\omega$ each period\n",
    "        Ew.append(np.dot([0, 1, 2], p1))\n",
    "\n",
    "            # add to the history vector of outcomes\n",
    "        out.append( outcome )\n",
    "\n",
    "            # add to the history of beliefs and update the prior\n",
    "            # save the effort that maximizes their success rate in the effort history vector\n",
    "        e.append( chose )\n",
    "\n",
    "        pw.append( p1 )\n",
    "\n",
    "        p0 = p1\n",
    "        \n",
    "    d = {'Ew': Ew[1:], 'choice': e, 'types':types[1:], 'outcomes': out, 'lr':likelihood[1:]}\n",
    "    df = pd.DataFrame(d)\n",
    "    dt.reset_index(inplace=True)\n",
    "    dt.rename\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37ae9583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_sim(w_true, type_true, T, N, M, pi_0, alpha):\n",
    "    df = pd.DataFrame({ \"index\":[],\n",
    "                             \"Ew_L\": [], \n",
    "                             \"choice_L\": [], \n",
    "                             \"types_L\":[], \n",
    "                             \"outcomes_L\": [], \n",
    "                             \"lr_L\":[],\n",
    "                       \n",
    "                             \"Ew_M\": [], \n",
    "                             \"choice_M\": [], \n",
    "                             \"types_M\":[], \n",
    "                             \"outcomes_M\": [], \n",
    "                             \"lr_M\":[],\n",
    "                       \n",
    "                             \"Ew_H\": [], \n",
    "                             \"choice_H\": [], \n",
    "                             \"types_H\":[], \n",
    "                             \"outcomes_H\": [], \n",
    "                             \"lr_H\":[]})\n",
    "    for n in range(N):\n",
    "        df_L = lr_indiv(w_true, type_true, 0, T, M, pi_0, alpha)\n",
    "        \n",
    "        df_L = df_L.rename(columns={ \n",
    "                             \"Ew\": \"Ew_L\", \n",
    "                             \"choice\": \"choice_L\", \n",
    "                             \"types\":\"types_L\", \n",
    "                             \"outcomes\": \"outcomes_L\", \n",
    "                             \"lr\":\"lr_L\"})\n",
    "        df_M = lr_indiv(w_true, type_true, 1, T, M, pi_0, alpha)\n",
    "        \n",
    "        df_M = df_M.rename(columns={\n",
    "                             \"Ew\": \"Ew_M\", \n",
    "                             \"choice\": \"choice_M\", \n",
    "                             \"types\":\"types_M\", \n",
    "                             \"outcomes\": \"outcomes_M\", \n",
    "                             \"lr\":\"lr_M\"})\n",
    "        df_H = lr_indiv(w_true, type_true, 2, T, M, pi_0, alpha)\n",
    "        \n",
    "        df_H = df_H.rename(columns={\n",
    "                             \"Ew\": \"Ew_H\", \n",
    "                             \"choice\": \"choice_H\", \n",
    "                             \"types\":\"types_H\", \n",
    "                             \"outcomes\": \"outcomes_H\", \n",
    "                             \"lr\":\"lr_H\"})\n",
    "        dt = pd.concat([df_L, df_M, df_H], axis = 1)\n",
    "        dt['id'] = n\n",
    "        dt.reset_index(inplace=True)\n",
    "        df = pd.concat([df, dt], ignore_index=True, sort=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94be0f8c",
   "metadata": {},
   "source": [
    "## Function to simulate the self serving attribution bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "050c11d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ss_simulation(w_true, type_true, T, N, M, pi_omega , pi_theta, gs_theta, gs_omega, gf_theta, gf_omega):\n",
    "    # gs and gf should be vectors with 3 elements. each element is the ss distortion parameter for low mid and high types \n",
    "    #respectively. s stands for a success and f for a failure. a gs_theta_3>1 is an attribution to the high type after a success.\n",
    "    # It should be accompanied by a gs_omega_3<1 which takes away weight from the high omega after a success. \n",
    "    #after failures, it shoulf be a high weight to low omega and a low weight to a low theta.\n",
    "    \n",
    "    pop_ss_omega = []\n",
    "    pop_ss_theta = []\n",
    "\n",
    "    pop_choices = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        \n",
    "        ss_omega = [pi_omega]\n",
    "        ss_theta = [pi_theta]\n",
    "        \n",
    "        p0_theta = pi_theta\n",
    "        p0_omega = pi_omega\n",
    "        \n",
    "        effort = []\n",
    "        \n",
    "        Ew = [np.dot([0, 1, 2], p0_omega)]\n",
    "        Eth = [np.dot([0, 1, 2], p0_theta)]\n",
    "        \n",
    "        outcomes = []\n",
    "        \n",
    "        for t in range(T):\n",
    "            \n",
    "            \n",
    "            #compute the expected success rate from every choice\n",
    "            Eu = p0_theta @ np.array([np.dot(M[0], p0_omega), np.dot(M[1], p0_omega), np.dot(M[2], p0_omega)])\n",
    "            \n",
    "            # save the effort that maximizes their success rate in the effort history vector\n",
    "            e = np.argmax( Eu )  \n",
    "\n",
    "            # Draw a realization using the probability under the true type, for a given true $\\omega$5\n",
    "            outcome = np.random.binomial(1, M[type_true][e, w_true], size=None)\n",
    "\n",
    "            # update the belief on omega given the outcome\n",
    "        \n",
    "            p1_theta, p1_omega = ss_update(gs_theta, gs_omega, gf_theta, gf_omega, p0_omega, p0_theta, e, outcome, M)\n",
    "            \n",
    "            # their expectation of $\\omega$ each period\n",
    "            Ew.append(np.dot([0, 1, 2], p1_omega))\n",
    "            Eth.append(np.dot([0, 1, 2], p1_theta))\n",
    "\n",
    "            # add to the history vector of outcomes\n",
    "            outcomes.append( outcome )\n",
    "\n",
    "            # add to the history of beliefs and update the prior\n",
    "            # save the effort that maximizes their success rate in the effort history vector\n",
    "            effort.append( e )\n",
    "\n",
    "            ss_theta.append( p1_theta )\n",
    "            ss_omega.append( p1_omega )\n",
    "\n",
    "            p0_theta = p1_theta\n",
    "            p0_omega = p1_omega\n",
    "\n",
    "        pop_ss_omega.append(Ew)\n",
    "        pop_ss_theta.append(Eth)\n",
    "\n",
    "        pop_choices.append(effort)\n",
    "        \n",
    "        \n",
    "\n",
    "    # turn each table into a data frame\n",
    "    df_ew = pd.DataFrame(pop_ss_omega)\n",
    "\n",
    "    df_choices = pd.DataFrame(pop_choices)\n",
    "\n",
    "    df_eth = pd.DataFrame(pop_ss_theta)\n",
    "\n",
    "    df_ew.columns = ['ew'+str(t) for t in range(T+1)]\n",
    "    \n",
    "    df_eth.columns = ['eth'+str(t) for t in range(T+1)]\n",
    "\n",
    "    df_choices.columns = ['choice'+str(t) for t in range(T)]\n",
    "\n",
    "    # go from multiple wide data frame to a single long one for $\\omega_L$\n",
    "    df = pd.concat([df_ew,\n",
    "                    df_eth,\n",
    "                    df_choices, \n",
    "                    ], axis=1)\n",
    "\n",
    "    df[\"id\"] = df.index\n",
    "\n",
    "    df = pd.wide_to_long(df, [\"ew\", \n",
    "                                \"eth\", \n",
    "                                \"choice\"], \n",
    "                             i=\"id\", j=\"year\")\n",
    "\n",
    "\n",
    "    return df\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca138a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_lr(df, cap):\n",
    "    df['truncated_lr_H']=df['lr_H']\n",
    "    df['truncated_lr_L']=df['lr_L']\n",
    "    df['truncated_lr_M']=df['lr_M']\n",
    "    df.loc[(df.truncated_lr_H > cap), 'truncated_lr_H'] = cap\n",
    "    df.loc[(df.truncated_lr_L > cap), 'truncated_lr_L'] = cap\n",
    "    df.loc[(df.truncated_lr_M > cap), 'truncated_lr_M'] = cap\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a67a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_averages(df, w_true):\n",
    "    \n",
    "    avg = df.groupby('year').mean()\n",
    "    \n",
    "    # Plot the time series of the average beliefs, effort and likelihood ratios  \n",
    "    plt.plot(avg['ew_L'], label = \"Low type\")\n",
    "    plt.plot(avg['ew_M'], label = \"Mid type\")\n",
    "    plt.plot(avg['ew_H'], label = \"High type\")\n",
    "    plt.plot(w_true*np.ones(len(avg['ew_L'])))\n",
    "\n",
    "    plt.title('Average Expected w by type')\n",
    "    plt.xlabel(\"Period\")\n",
    "    plt.ylabel(\"w\")\n",
    "    plt.legend()\n",
    "    plt.show()  \n",
    "\n",
    "    plt.plot(avg['choice_L'], label = \"Low type\")\n",
    "    plt.plot(avg['choice_M'], label = \"Mid type\")\n",
    "    plt.plot(avg['choice_H'], label = \"High type\")\n",
    "    plt.plot(w_true*np.ones(len(avg['ew_L'])))\n",
    "    \n",
    "    plt.title('Average Choices by type')\n",
    "    plt.xlabel(\"Period\")\n",
    "    plt.ylabel(\"Average Effort\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(avg['lr_L'], label = \"Low type\")\n",
    "    plt.plot(avg['lr_M'], label = \"Mid type\")\n",
    "    plt.plot(avg['lr_H'], label = \"High type\")\n",
    "\n",
    "    plt.title('Average Likelihood ratio by type (true/model)')\n",
    "    plt.xlabel(\"Period\")\n",
    "    plt.ylim([0, 2])\n",
    "    plt.ylabel(\"Average Likelihood Ratio\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2ff8b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(df, periods):\n",
    "# histograms of the choices in 4 periods\n",
    "    x1 = [df.loc[(slice(None), periods[0]), 'choice_L'], df.loc[(slice(None), periods[0]), 'choice_M'], df.loc[(slice(None), periods[0]), 'choice_H']]\n",
    "    x2 = [df.loc[(slice(None), periods[1]), 'choice_L'], df.loc[(slice(None), periods[1]), 'choice_M'], df.loc[(slice(None), periods[1]), 'choice_H']]\n",
    "    x3 = [df.loc[(slice(None), periods[2]), 'choice_L'], df.loc[(slice(None), periods[2]), 'choice_M'], df.loc[(slice(None), periods[2]), 'choice_H']]\n",
    "    x4 = [df.loc[(slice(None), periods[3]), 'choice_L'], df.loc[(slice(None), periods[3]), 'choice_M'], df.loc[(slice(None), periods[3]), 'choice_H']]\n",
    "\n",
    "    n_bins = [0, 1, 2, 3]\n",
    "\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "    fig.suptitle('Distribution of choices')\n",
    "\n",
    "    fig.set_figheight(10)\n",
    "    fig.set_figwidth(15)\n",
    "\n",
    "    ax1.hist(x1, n_bins, density=True, histtype='bar', label=['Low type', 'Mid type', 'High type'], align='left')\n",
    "    ax1.legend(prop={'size': 10})\n",
    "    ax1.set_title('t = ' + str(periods[0]))\n",
    "\n",
    "    ax2.hist(x2, n_bins, density=True, histtype='bar', label=['Low type', 'Mid type', 'High type'], align='left')\n",
    "    ax2.legend(prop={'size': 10})\n",
    "    ax2.set_title('t = ' + str(periods[1]))\n",
    "\n",
    "    ax3.hist(x3, n_bins, density=True, histtype='bar', label=['Low type', 'Mid type', 'High type'], align='left')\n",
    "    ax3.legend(prop={'size': 10})\n",
    "    ax3.set_title('t = ' + str(periods[2]))\n",
    "\n",
    "    ax4.hist(x4, n_bins, density=True, histtype='bar', label=['Low type', 'Mid type', 'High type'], align='left')\n",
    "    ax4.legend(prop={'size': 10})\n",
    "    ax4.set_title('t = ' + str(periods[3]))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "816328ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pdf(df, var, cap, periods):\n",
    "    if var == 'lr':\n",
    "        var = 'truncated_lr'\n",
    "    else:\n",
    "        var = var\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    fig.suptitle('distribution of '+ var + ' on different periods')\n",
    "\n",
    "    sns.kdeplot(ax=axes[0, 0], data=df.loc[(slice(None), periods[0]), var+'_L'],shade=True, color=\"blue\", label='Low type')\n",
    "    sns.kdeplot(ax=axes[0, 0], data=df.loc[(slice(None), periods[0]), var+'_M'],shade=True, color=\"orange\", label='Mid type')\n",
    "    sns.kdeplot(ax=axes[0, 0], data=df.loc[(slice(None), periods[0]), var+'_H'],shade=True, color=\"green\", label='High type')\n",
    "    axes[0,0].legend (loc = 'upper right')\n",
    "    axes[0,0].set_title('t = ' + str(periods[0]))\n",
    "    axes[0,0].set_xlim(0, cap)\n",
    "\n",
    "\n",
    "    sns.kdeplot(ax=axes[0, 1], data=df.loc[(slice(None), periods[1]), var+'_L'],shade=True, color=\"blue\", label='Low type')\n",
    "    sns.kdeplot(ax=axes[0, 1], data=df.loc[(slice(None), periods[1]), var+'_M'],shade=True, color=\"orange\", label='Mid type')\n",
    "    sns.kdeplot(ax=axes[0, 1], data=df.loc[(slice(None), periods[1]), var+'_H'],shade=True, color=\"green\", label='High type')\n",
    "    axes[0,1].legend (loc = 'upper right')\n",
    "    axes[0,1].set_title('t = ' + str(periods[1]))\n",
    "    axes[0,1].set_xlim(0, cap)\n",
    "\n",
    "    sns.kdeplot(ax=axes[1, 0], data=df.loc[(slice(None), periods[2]), var+'_L'],shade=True, color=\"blue\", label='Low type')\n",
    "    sns.kdeplot(ax=axes[1, 0], data=df.loc[(slice(None), periods[2]), var+'_M'],shade=True, color=\"orange\", label='Mid type')\n",
    "    sns.kdeplot(ax=axes[1, 0], data=df.loc[(slice(None), periods[2]), var+'_H'],shade=True, color=\"green\", label='High type')\n",
    "    axes[1,0].legend (loc = 'upper right')\n",
    "    axes[1,0].set_title('t = ' + str(periods[2]))\n",
    "    axes[1,0].set_xlim(0, cap)\n",
    "    axes[1,0].set_ylim(0, 0.9)\n",
    "\n",
    "    sns.kdeplot(ax=axes[1, 1], data=df.loc[(slice(None), periods[3]), var+'_L'],shade=True, color=\"blue\", label='Low type')\n",
    "    sns.kdeplot(ax=axes[1, 1], data=df.loc[(slice(None), periods[3]), var+'_M'],shade=True, color=\"orange\", label='Mid type')\n",
    "    sns.kdeplot(ax=axes[1, 1], data=df.loc[(slice(None), periods[3]), var+'_H'],shade=True, color=\"green\", label='High type')\n",
    "    axes[1,1].legend (loc = 'upper right')\n",
    "    axes[1,1].set_title('t = ' + str(periods[3]))\n",
    "    axes[1,1].set_xlim(0, cap)\n",
    "    axes[1,1].set_ylim(0, 0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "896df6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_CDF(df, var, periods, alpha, cap):\n",
    "    \n",
    "    if var == 'lr':\n",
    "        var = 'truncated_lr'\n",
    "    else:\n",
    "        var = var\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    alpha_threshold = [[alpha, alpha], [0, 1]]\n",
    "\n",
    "    fig.suptitle('CDF of ' + var + ' on different periods')\n",
    "    \n",
    "    sns.ecdfplot(ax=axes[0, 0], data=df.loc[(slice(None), periods[0]), var+'_L'], color=\"blue\", label='Low type')\n",
    "    sns.ecdfplot(ax=axes[0, 0], data=df.loc[(slice(None), periods[0]), var+'_M'], color=\"orange\", label='Mid type')\n",
    "    sns.ecdfplot(ax=axes[0, 0], data=df.loc[(slice(None), periods[0]), var+'_H'], color=\"green\", label='High type')\n",
    "    axes[0,0].legend (loc = 'upper right')\n",
    "    axes[0,0].set_title('t = ' + str(periods[0]))\n",
    "    axes[0,0].set_xlim(0, cap + .5 )\n",
    "    axes[0,0].set_ylim(0, 1)\n",
    "\n",
    "\n",
    "    sns.ecdfplot(ax=axes[0, 1], data=df.loc[(slice(None), periods[1]), var+'_L'], color=\"blue\", label='Low type')\n",
    "    sns.ecdfplot(ax=axes[0, 1], data=df.loc[(slice(None), periods[1]), var+'_M'], color=\"orange\", label='Mid type')\n",
    "    sns.ecdfplot(ax=axes[0, 1], data=df.loc[(slice(None), periods[1]), var+'_H'], color=\"green\", label='High type')\n",
    "    axes[0,1].legend (loc = 'upper right')\n",
    "    axes[0,1].set_title('t = ' + str(periods[1]))\n",
    "    axes[0,1].set_xlim(0, cap + .5)\n",
    "    axes[0,1].set_ylim(0, 1)\n",
    "\n",
    "    sns.ecdfplot(ax=axes[1, 0], data=df.loc[(slice(None), periods[2]), var+'_L'], color=\"blue\", label='Low type')\n",
    "    sns.ecdfplot(ax=axes[1, 0], data=df.loc[(slice(None), periods[2]), var+'_M'], color=\"orange\", label='Mid type')\n",
    "    sns.ecdfplot(ax=axes[1, 0], data=df.loc[(slice(None), periods[2]), var+'_H'], color=\"green\", label='High type')\n",
    "    axes[1,0].legend (loc = 'upper right')\n",
    "    axes[1,0].set_title('t = ' + str(periods[2]))\n",
    "    axes[1,0].set_xlim(0, cap + .5)\n",
    "    axes[1,0].set_ylim(0, 1)\n",
    "\n",
    "    sns.ecdfplot(ax=axes[1, 1], data=df.loc[(slice(None), periods[3]), var+'_L'], color=\"blue\", label='Low type')\n",
    "    sns.ecdfplot(ax=axes[1, 1], data=df.loc[(slice(None), periods[3]), var+'_M'], color=\"orange\", label='Mid type')\n",
    "    sns.ecdfplot(ax=axes[1, 1], data=df.loc[(slice(None), periods[3]), var+'_H'], color=\"green\", label='High type')\n",
    "    axes[1,1].legend (loc = 'upper right')\n",
    "    axes[1,1].set_title('t = ' + str(periods[3]))\n",
    "    axes[1,1].set_xlim(0, cap + .5)\n",
    "    axes[1,1].set_ylim(0, 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4ff263",
   "metadata": {},
   "source": [
    "# Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255e1bb3",
   "metadata": {},
   "source": [
    "### Set some values that will be used throughout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b6b9167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior over omega\n",
    "pi_0 = [1/3, 1/3, 1/3]\n",
    "\n",
    "# cap for the likelihood ratio\n",
    "cap = 3\n",
    "\n",
    "# number of periods\n",
    "T = 100\n",
    "\n",
    "# number of agents\n",
    "N = 1000\n",
    "\n",
    "# the probability matrices\n",
    "ML = np.array([[.30, .40, .55], [.20, .55, .65], [0, .40, .75]])\n",
    "MM = np.array([[.35, .45, .60], [.25, .60, .70], [.05, .45, .80]])\n",
    "MH = np.array([[.40, .50, .65], [.30, .65, .80], [.10, .50, .95]])\n",
    "\n",
    "M = [ML, MM, MH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01be95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an alternative probability matrix\n",
    "ML1 = np.array([[.10, .30, .45], [.05, .40, .6], [0, .30, .75]])\n",
    "MM1 = np.array([[.15, .35, .50], [.10, .45, .65], [.05, .35, .80]])\n",
    "MH1 = np.array([[.20, .40, .55], [.15, .50, .75], [.10, .40, .95]])\n",
    "\n",
    "M1 = [ML1, MM1, MH1]\n",
    "\n",
    "#M0 are the matrices with probabilities from the function above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce946f17",
   "metadata": {},
   "source": [
    "### Case 1: When the true type is $\\theta_L$\n",
    "\n",
    "## Predictions\n",
    "\n",
    "### In Heidhues, Koszegi and Strack (2018):\n",
    "This case has two different types that may be overconfident. That means they will be disappointed with the outcomes and they will update their belief about $\\omega$ downwards. This means they will update in the right direction and end up choosing the correct action. The most overconfident agents---those with a model $\\theta_H$---will be the most disappointed and thus will choose the low effort a the highest rates. However, if we were to have lower values for $\\omega$ they would like to update even further down that the true value. They are choosing the true action simply because they are constrained at that corner.\n",
    "In contrast, the types who have the right model will be the last to learn the true exchange rate because they are not as disappointed as the other types\n",
    "\n",
    "### In Ba (2022, wp):\n",
    "If the agents start off with a dogmatic belief that they are a type $\\theta>\\theta^*$ (So in this case both the $\\theta_H$ and the $\\theta_M$ models) and consider the truth ($\\theta_L$) as the only alternative model, we see that because the ones with model $\\theta_H$ are so disappointed, they will start believeing that their model is wrong very fast and update to the trutha at higher rates than those with the model $\\theta_M$. \n",
    "\n",
    "In the simulated date we see that about 30% of the agents with the $\\theta_M$ model do not change their model for the true one. However, they do choose the right action. Again the fact that they make the right choice is because they are constrained from below. If there were lower values of $\\omega$, they would update further down and choose a lower effort.\n",
    "\n",
    "\n",
    "### In Coutts, Gherards and Murad (2022 wp):\n",
    "Need to simulate this part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307d8497",
   "metadata": {},
   "source": [
    "## For a true exchange rate of $\\omega_L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80615c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_00 = sim_data(0, 0, pi_0, 100, 1000, [ML, MM, MH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736e51e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_00 = truncate_lr(df_00, 3)\n",
    "plot_averages(df_00, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64205111",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(df_00, [0, 10, 50, 99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef24542",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdf(df_00, 'lr', 3, [1, 10, 50, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a32d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_CDF(df_00, 'lr', [1, 10, 50, 100], 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d195d7ee",
   "metadata": {},
   "source": [
    "## For a true exchange rate of $\\omega_M$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cac9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10 = sim_data(1, 0, pi_0, 100, 1000, [ML, MM, MH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2841174",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10 = truncate_lr(df_10, 3)\n",
    "plot_averages(df_10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eab6d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(df_10, [1, 9, 49, 99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e76c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdf(df_10, 'lr', 3, [1, 10, 50, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d794f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_CDF(df_10, 'lr', [1, 10, 50, 100], 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1268d224",
   "metadata": {},
   "source": [
    "## For a true exchange rate of $\\omega_H$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6aec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20 = sim_data(2, 0, pi_0, 100, 1000, [ML, MM, MH])\n",
    "\n",
    "df_20 = truncate_lr(df_20, 3)\n",
    "plot_averages(df_20, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e91a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(df_20, [0, 9, 49, 99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fe49fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdf(df_20, 'lr', 3, [1, 10, 50, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468a775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_CDF(df_20, 'lr', [1, 10, 50, 100], 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72711709",
   "metadata": {},
   "source": [
    "# Case 2: When the true type is $\\theta_M$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a56802",
   "metadata": {},
   "source": [
    "## For a true exchange rate of $\\omega_L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e95b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_01 = sim_data(0, 1, pi_0, 100, 1000, [ML, MM, MH])\n",
    "\n",
    "df_01 = truncate_lr(df_01, 3)\n",
    "plot_averages(df_01, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513b8a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(df_01, [0, 9, 49, 99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a437515",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdf(df_01, 'lr', 3, [1, 10, 50, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba29c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_CDF(df_01, 'lr', [1, 10, 50, 100], 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4e9642",
   "metadata": {},
   "source": [
    "## For a true exchange rate of $\\omega_M$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ade8c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_11 = sim_data(1, 1, pi_0, 100, 1000, [ML, MM, MH])\n",
    "\n",
    "df_11 = truncate_lr(df_11, 3)\n",
    "plot_averages(df_11, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f87988",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(df_11, [0, 9, 49, 99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770c02bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdf(df_11, 'lr', 3, [1, 10, 50, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6ce127",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_CDF(df_11, 'lr', [1, 10, 50, 100], 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdce0223",
   "metadata": {},
   "source": [
    "## For a true exchange rate of $\\omega_H$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd86b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_21 = sim_data(2, 1, pi_0, 100, 1000, [ML, MM, MH])\n",
    "\n",
    "df_21 = truncate_lr(df_21, 3)\n",
    "plot_averages(df_21, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf2eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(df_21, [0, 9, 49, 99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f832f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdf(df_21, 'lr', 3, [1, 10, 50, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bab23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_CDF(df_21, 'lr', [1, 10, 50, 100], 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15b89a4",
   "metadata": {},
   "source": [
    "# Case 3: When the true typr is $\\theta_H$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ad928f",
   "metadata": {},
   "source": [
    "## For a true exchange rate of $\\omega_L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faedc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_02 = sim_data(0, 2, pi_0, 100, 1000, [ML, MM, MH])\n",
    "\n",
    "df_02 = truncate_lr(df_02, 3)\n",
    "plot_averages(df_02, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(df_02, [0, 9, 49, 99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8493a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdf(df_02, 'lr', 3, [1, 10, 50, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f2b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_CDF(df_02, 'lr', [1, 10, 50, 100], 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcacc5f8",
   "metadata": {},
   "source": [
    "## For a true exchange rate of $\\omega_M$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dea3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_12 = sim_data(1, 2, pi_0, 100, 1000, [ML, MM, MH])\n",
    "\n",
    "df_12 = truncate_lr(df_12, 3)\n",
    "plot_averages(df_12, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f52587",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(df_12, [0, 9, 49, 99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f81cae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdf(df_12, 'lr', 3, [1, 10, 50, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a5e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_CDF(df_12, 'lr', [1, 10, 50, 100], 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddd32cb",
   "metadata": {},
   "source": [
    "## For a true exchange rate of $\\omega_H$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa7ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_22 = sim_data(2, 2, pi_0, 100, 1000, [ML, MM, MH])\n",
    "\n",
    "df_22 = truncate_lr(df_22, 3)\n",
    "plot_averages(df_22, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2646f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(df_22, [0, 9, 49, 99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec924db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdf(df_22, 'lr', 3, [1, 10, 50, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5190ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_CDF(df_22, 'lr', [1, 10, 50, 100], 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300f9b1c",
   "metadata": {},
   "source": [
    "# The Bayesian update with and without self serving attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecce8ee",
   "metadata": {},
   "source": [
    "# Simulation with Self-Serving Attribution bias and fully bayesian with uniform priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745acd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_00 = ss_simulation(0, 0, 100, 1000, [ML, MM, MH], [1/3, 1/3, 1/3] , [1/3, 1/3, 1/3], [1, 1, 1.2], [1, 1, .8], [.8, 1, 1], [1.2, 1, 1])\n",
    "bay_00 = ss_simulation(0, 0, 100, 1000, [ML, MM, MH], [1/3, 1/3, 1/3] , [1/3, 1/3, 1/3], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1])\n",
    "\n",
    "ss_10 = ss_simulation(1, 0, 100, 1000, [ML, MM, MH], [1/3, 1/3, 1/3] , [1/3, 1/3, 1/3], [1, 1, 1.2], [1, 1, .8], [.8, 1, 1], [1.2, 1, 1])\n",
    "bay_10 = ss_simulation(1, 0, 100, 1000, [ML, MM, MH], [1/3, 1/3, 1/3] , [1/3, 1/3, 1/3], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1])\n",
    "\n",
    "ss_20 = ss_simulation(2, 0, 100, 1000, [ML, MM, MH], [1/3, 1/3, 1/3] , [1/3, 1/3, 1/3], [1, 1, 1.2], [1, 1, .8], [.8, 1, 1], [1.2, 1, 1])\n",
    "bay_20 = ss_simulation(2, 0, 100, 1000, [ML, MM, MH], [1/3, 1/3, 1/3] , [1/3, 1/3, 1/3], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1])\n",
    "\n",
    "ss_01 = ss_simulation(0, 1, 100, 1000, [ML, MM, MH], [1/3, 1/3, 1/3] , [1/3, 1/3, 1/3], [1, 1, 1.2], [1, 1, .8], [.8, 1, 1], [1.2, 1, 1])\n",
    "bay_01 = ss_simulation(0, 1, 100, 1000, [ML, MM, MH], [1/3, 1/3, 1/3] , [1/3, 1/3, 1/3], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1])\n",
    "\n",
    "ss_11 = ss_simulation(1, 1, 100, 1000, [ML, MM, MH], [1/3, 1/3, 1/3] , [1/3, 1/3, 1/3], [1, 1, 1.2], [1, 1, .8], [.8, 1, 1], [1.2, 1, 1])\n",
    "bay_11 = ss_simulation(1, 1, 100, 1000, [ML, MM, MH], [1/3, 1/3, 1/3] , [1/3, 1/3, 1/3], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1])\n",
    "\n",
    "ss_21 = ss_simulation(2, 1, 100, 1000, [ML, MM, MH], [1/3, 1/3, 1/3] , [1/3, 1/3, 1/3], [1, 1, 1.2], [1, 1, .8], [.8, 1, 1], [1.2, 1, 1])\n",
    "bay_21 = ss_simulation(2, 1, 100, 1000, [ML, MM, MH], [1/3, 1/3, 1/3] , [1/3, 1/3, 1/3], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1])\n",
    "\n",
    "ss_02 = ss_simulation(0, 2, 100, 1000, [ML, MM, MH], [1/3, 1/3, 1/3] , [1/3, 1/3, 1/3], [1, 1, 1.2], [1, 1, .8], [.8, 1, 1], [1.2, 1, 1])\n",
    "bay_02 = ss_simulation(0, 2, 100, 1000, [ML, MM, MH], [1/3, 1/3, 1/3] , [1/3, 1/3, 1/3], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1])\n",
    "\n",
    "ss_12 = ss_simulation(1, 2, 100, 1000, [ML, MM, MH], [1/3, 1/3, 1/3] , [1/3, 1/3, 1/3], [1, 1, 1.2], [1, 1, .8], [.8, 1, 1], [1.2, 1, 1])\n",
    "bay_12 = ss_simulation(1, 2, 100, 1000, [ML, MM, MH], [1/3, 1/3, 1/3] , [1/3, 1/3, 1/3], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1])\n",
    "\n",
    "ss_22 = ss_simulation(2, 2, 100, 1000, [ML, MM, MH], [1/3, 1/3, 1/3] , [1/3, 1/3, 1/3], [1, 1, 1.2], [1, 1, .8], [.8, 1, 1], [1.2, 1, 1])\n",
    "bay_22 = ss_simulation(2, 2, 100, 1000, [ML, MM, MH], [1/3, 1/3, 1/3] , [1/3, 1/3, 1/3], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b046bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_00.reset_index(inplace=True)\n",
    "df_10.reset_index(inplace=True)\n",
    "df_20.reset_index(inplace=True)\n",
    "df_01.reset_index(inplace=True)\n",
    "df_11.reset_index(inplace=True)\n",
    "df_21.reset_index(inplace=True)\n",
    "df_02.reset_index(inplace=True)\n",
    "df_12.reset_index(inplace=True)\n",
    "df_22.reset_index(inplace=True)\n",
    "\n",
    "ss_00.reset_index(inplace=True)\n",
    "ss_10.reset_index(inplace=True)\n",
    "ss_20.reset_index(inplace=True)\n",
    "ss_01.reset_index(inplace=True)\n",
    "ss_11.reset_index(inplace=True)\n",
    "ss_21.reset_index(inplace=True)\n",
    "ss_02.reset_index(inplace=True)\n",
    "ss_12.reset_index(inplace=True)\n",
    "ss_22.reset_index(inplace=True)\n",
    "\n",
    "bay_00.reset_index(inplace=True)\n",
    "bay_10.reset_index(inplace=True)\n",
    "bay_20.reset_index(inplace=True)\n",
    "bay_01.reset_index(inplace=True)\n",
    "bay_11.reset_index(inplace=True)\n",
    "bay_21.reset_index(inplace=True)\n",
    "bay_02.reset_index(inplace=True)\n",
    "bay_12.reset_index(inplace=True)\n",
    "bay_22.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffdbdb7",
   "metadata": {},
   "source": [
    "# Simulation of the choices with a likelihood ratio test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c4f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.05\n",
    "lr_00 = lr_sim(0, 0, 100, 1000, [ML, MM, MH], pi_0, alpha)\n",
    "lr_10 = lr_sim(1, 0, 100, 1000, [ML, MM, MH], pi_0, alpha)\n",
    "lr_20 = lr_sim(2, 0, 100, 1000, [ML, MM, MH], pi_0, alpha)\n",
    "\n",
    "lr_01 = lr_sim(0, 1, 100, 1000, [ML, MM, MH], pi_0, alpha)\n",
    "lr_11 = lr_sim(1, 1, 100, 1000, [ML, MM, MH], pi_0, alpha)\n",
    "lr_21 = lr_sim(2, 1, 100, 1000, [ML, MM, MH], pi_0, alpha)\n",
    "\n",
    "lr_02 = lr_sim(0, 2, 100, 1000, [ML, MM, MH], pi_0, alpha)\n",
    "lr_12 = lr_sim(1, 2, 100, 1000, [ML, MM, MH], pi_0, alpha)\n",
    "lr_22 = lr_sim(2, 2, 100, 1000, [ML, MM, MH], pi_0, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee0814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_00['omega'] = 0\n",
    "lr_00['theta'] = 0\n",
    "\n",
    "lr_10['omega'] = 1\n",
    "lr_10['theta'] = 0\n",
    "\n",
    "lr_20['omega'] = 2\n",
    "lr_20['theta'] = 0\n",
    "\n",
    "lr_01['omega'] = 0\n",
    "lr_01['theta'] = 1\n",
    "\n",
    "lr_11['omega'] = 1\n",
    "lr_11['theta'] = 1\n",
    "\n",
    "lr_21['omega'] = 2\n",
    "lr_21['theta'] = 1\n",
    "\n",
    "lr_02['omega'] = 0\n",
    "lr_02['theta'] = 2\n",
    "\n",
    "lr_12['omega'] = 1\n",
    "lr_12['theta'] = 2\n",
    "\n",
    "lr_22['omega'] = 2\n",
    "lr_22['theta'] = 2\n",
    "lr_a10 = pd.concat([lr_00, lr_10, lr_20, lr_01, lr_11, lr_21, lr_02, lr_12, lr_22], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120fa461",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_00['omega'] = 0\n",
    "ss_00['theta'] = 0\n",
    "\n",
    "ss_10['omega'] = 1\n",
    "ss_10['theta'] = 0\n",
    "\n",
    "ss_20['omega'] = 2\n",
    "ss_20['theta'] = 0\n",
    "\n",
    "ss_01['omega'] = 0\n",
    "ss_01['theta'] = 1\n",
    "\n",
    "ss_11['omega'] = 1\n",
    "ss_11['theta'] = 1\n",
    "\n",
    "ss_21['omega'] = 2\n",
    "ss_21['theta'] = 1\n",
    "\n",
    "ss_02['omega'] = 0\n",
    "ss_02['theta'] = 2\n",
    "\n",
    "ss_12['omega'] = 1\n",
    "ss_12['theta'] = 2\n",
    "\n",
    "ss_22['omega'] = 2\n",
    "ss_22['theta'] = 2\n",
    "\n",
    "ss_gH20 = pd.concat([ss_00, ss_10, ss_20, ss_01, ss_11, ss_21, ss_02, ss_12, ss_22], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f547e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_00['omega'] = 0\n",
    "df_00['theta'] = 0\n",
    "\n",
    "df_10['omega'] = 1\n",
    "df_10['theta'] = 0\n",
    "\n",
    "df_20['omega'] = 2\n",
    "df_20['theta'] = 0\n",
    "\n",
    "df_01['omega'] = 0\n",
    "df_01['theta'] = 1\n",
    "\n",
    "df_11['omega'] = 1\n",
    "df_11['theta'] = 1\n",
    "\n",
    "df_21['omega'] = 2\n",
    "df_21['theta'] = 1\n",
    "\n",
    "df_02['omega'] = 0\n",
    "df_02['theta'] = 2\n",
    "\n",
    "df_12['omega'] = 1\n",
    "df_12['theta'] = 2\n",
    "\n",
    "df_22['omega'] = 2\n",
    "df_22['theta'] = 2\n",
    "dd = pd.concat([df_00, df_10, df_20, df_01, df_11, df_21, df_02, df_12, df_22], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f3b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_00['omega'] = 0\n",
    "bay_00['theta'] = 0\n",
    "\n",
    "bay_10['omega'] = 1\n",
    "bay_10['theta'] = 0\n",
    "\n",
    "bay_20['omega'] = 2\n",
    "bay_20['theta'] = 0\n",
    "\n",
    "bay_01['omega'] = 0\n",
    "bay_01['theta'] = 1\n",
    "\n",
    "bay_11['omega'] = 1\n",
    "bay_11['theta'] = 1\n",
    "\n",
    "bay_21['omega'] = 2\n",
    "bay_21['theta'] = 1\n",
    "\n",
    "bay_02['omega'] = 0\n",
    "bay_02['theta'] = 2\n",
    "\n",
    "bay_12['omega'] = 1\n",
    "bay_12['theta'] = 2\n",
    "\n",
    "bay_22['omega'] = 2\n",
    "bay_22['theta'] = 2\n",
    "bay = pd.concat([bay_00, bay_10, bay_20, bay_01, bay_11, bay_21, bay_02, bay_12, bay_22], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21630ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_time(omega, theta):    \n",
    "    sns.set_theme(style=\"ticks\")\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 7))\n",
    "    sns.despine(fig)\n",
    "    \n",
    "    pal = \"Accent\"\n",
    "\n",
    "    sns.histplot(ax=axes[0, 0],\n",
    "        data=dd[(dd.omega == omega) & (dd.theta == theta)],\n",
    "        x=\"year\", hue=\"choice_L\",\n",
    "        multiple=\"stack\",\n",
    "        palette=pal,\n",
    "        edgecolor=\".3\",\n",
    "        linewidth=.1,\n",
    "        binwidth=1,\n",
    "        binrange=[0, 100]\n",
    "    )\n",
    "\n",
    "    sns.histplot(ax=axes[0, 1],\n",
    "        data=dd[(dd.omega == omega) & (dd.theta == theta)],\n",
    "        x=\"year\", hue=\"choice_M\",\n",
    "        multiple=\"stack\",\n",
    "        palette=pal,\n",
    "        edgecolor=\".3\",\n",
    "        linewidth=.1,\n",
    "        binwidth=1,\n",
    "        binrange=[0, 100]\n",
    "    )\n",
    "\n",
    "    sns.histplot(ax=axes[0, 2],\n",
    "        data=dd[(dd.omega == omega) & (dd.theta == theta)],\n",
    "        x=\"year\", hue=\"choice_H\",\n",
    "        multiple=\"stack\",\n",
    "        palette=pal,\n",
    "        edgecolor=\".3\",\n",
    "        linewidth=.1,\n",
    "        binwidth=1,\n",
    "        binrange=[0, 100]\n",
    "    )\n",
    "\n",
    "    sns.histplot(ax=axes[1, 0],\n",
    "        data=lr_a10[(lr_a10.omega == omega) & (lr_a10.theta == theta)],\n",
    "        x=\"index\", hue=\"choice_L\",\n",
    "        multiple=\"stack\",\n",
    "        palette=pal,\n",
    "        edgecolor=\".3\",\n",
    "        linewidth=.1,\n",
    "        binwidth=1,\n",
    "        binrange=[0, 100]\n",
    "    )\n",
    "    \n",
    "    sns.histplot(ax=axes[1, 1],\n",
    "        data=lr_a10[(lr_a10.omega == omega) & (lr_a10.theta==theta)],\n",
    "        x=\"index\", hue=\"choice_M\",\n",
    "        multiple=\"stack\",\n",
    "        palette=pal,\n",
    "        edgecolor=\".3\",\n",
    "        linewidth=.1,\n",
    "        binwidth=1,\n",
    "        binrange=[0, 100]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    sns.histplot(ax=axes[1, 2],\n",
    "        data=lr_a10[(lr_a10.omega == omega) & (lr_a10.theta==theta)],\n",
    "        x=\"index\", hue=\"choice_H\",\n",
    "        multiple=\"stack\",\n",
    "        palette=pal,\n",
    "        edgecolor=\".3\",\n",
    "        linewidth=.1,\n",
    "        binwidth=1,\n",
    "        binrange=[0, 100]\n",
    "    )\n",
    "   \n",
    "    \n",
    "    sns.histplot(ax=axes[2, 0],\n",
    "        data=ss_gH20[(ss_gH20.omega == omega) & (ss_gH20.theta==theta)],\n",
    "        x=\"year\", hue=\"choice\",\n",
    "        multiple=\"stack\",\n",
    "        palette=pal,\n",
    "        edgecolor=\".3\",\n",
    "        linewidth=.1,\n",
    "        binwidth=1,\n",
    "        binrange=[0, 100]\n",
    "    )\n",
    "    \n",
    "    sns.histplot(ax=axes[2, 1],\n",
    "        data=bay[(bay.omega == omega) & (bay.theta==theta)],\n",
    "        x=\"year\", hue=\"choice\",\n",
    "        multiple=\"stack\",\n",
    "        palette=pal,\n",
    "        edgecolor=\".3\",\n",
    "        linewidth=.1,\n",
    "        binwidth=1,\n",
    "        binrange=[0, 100]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081489f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_time(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02774ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_time(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42022d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_time(2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb5dbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_time(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6abcad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_time(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022b4036",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_time(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c0026",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_time(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c47c689",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_time(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6594bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_time(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218243fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_time(omega, theta, n):    \n",
    "    sns.set_theme(style=\"ticks\")\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 7))\n",
    "    sns.despine(fig)\n",
    "    \n",
    "    pal = \"flare\"\n",
    "\n",
    "    sns.lineplot(ax=axes[0, 0],\n",
    "        data=dd[(dd.omega == omega) & (dd.theta == theta) & (dd.id < n)],\n",
    "        x=\"year\", y=\"choice_L\", hue=\"id\",\n",
    "        palette=pal, lw=1, estimator=None\n",
    "    )\n",
    "\n",
    "    sns.lineplot(ax=axes[0, 1],\n",
    "        data=dd[(dd.omega == omega) & (dd.theta == theta) & (dd.id < n)],\n",
    "        x=\"year\", y=\"choice_M\", hue = \"id\",\n",
    "        palette=pal, lw=1, estimator=None\n",
    "    )\n",
    "\n",
    "    sns.lineplot(ax=axes[0, 2],\n",
    "        data=dd[(dd.omega == omega) & (dd.theta == theta) & (dd.id < n)],\n",
    "        x=\"year\", y=\"choice_H\", hue=\"id\",\n",
    "        palette=pal, lw=1, estimator=None\n",
    "    )\n",
    "\n",
    "    sns.lineplot(ax=axes[1, 0],\n",
    "        data=lr_a10[(lr_a10.omega == omega) & (lr_a10.theta == theta) & (lr_a10.id < n)],\n",
    "        x=\"index\", y=\"choice_L\", hue = \"id\",\n",
    "        palette=pal, lw=1, estimator=None\n",
    "    )\n",
    "    \n",
    "    \n",
    "    sns.lineplot(ax=axes[1, 1],\n",
    "        data=lr_a10[(lr_a10.omega == omega) & (lr_a10.theta == theta) & (lr_a10.id < n)],\n",
    "        x=\"index\", y=\"choice_M\", hue=\"id\",\n",
    "        palette=pal, lw=1, estimator=None\n",
    "    )\n",
    "    \n",
    "    \n",
    "    sns.lineplot(ax=axes[1, 2],\n",
    "        data=lr_a10[(lr_a10.omega == omega) & (lr_a10.theta == theta) & (lr_a10.id < n)],\n",
    "        x=\"index\", y=\"choice_H\", hue='id',\n",
    "        palette=pal, lw=1, estimator=None\n",
    "    )\n",
    "   \n",
    "    \n",
    "    sns.lineplot(ax=axes[2, 0],\n",
    "        data=ss_gH20[(ss_gH20.omega == omega) & (ss_gH20.theta==theta) & (ss_gH20.id < n)],\n",
    "        x=\"year\", y=\"choice\", hue='id',\n",
    "        palette=pal, lw=1, estimator=None\n",
    "    )\n",
    "    \n",
    "    sns.lineplot(ax=axes[2, 1],\n",
    "        data=bay[(bay.omega == omega) & (bay.theta == theta) & (bay.id < n)],\n",
    "        x=\"year\", y=\"choice\", hue='id',\n",
    "        palette=pal, lw=1, estimator=None\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fea2241",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_time(0, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25825f89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
